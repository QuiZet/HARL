{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "observation construct when there are $x$ number of adversaries, $y$ number of good_agents and $z$ number of landmarks:\\\n",
    "adversary $= 4+2z+2(x-1)+4y$\\\n",
    "good_agent $= 4+2z+2x+4(y-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "# Import PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import PyTorch geometric libraries for GNNs\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "\n",
    "# Import PettingZoo and Simple Tag environment\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "\n",
    "# Import gym for action space\n",
    "import gym\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modules for simple_tag_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Observation wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationWrapper:\n",
    "    def __init__(self, env, num_class_A, num_class_B):\n",
    "        self.env = env\n",
    "        self.num_class_A = num_class_A\n",
    "        self.num_class_B = num_class_B\n",
    "        self.adversaries = [agent for agent in env.agents if 'adversary' in agent]\n",
    "        self.good_agents = [agent for agent in env.agents if 'agent' in agent]\n",
    "        self.landmarks = self._get_landmarks()\n",
    "\n",
    "        # Assign classes to adversaries\n",
    "        self.class_A_adversaries = self.adversaries[:num_class_A]\n",
    "        self.class_B_adversaries = self.adversaries[num_class_A:num_class_A+num_class_B]\n",
    "        self.adversary_classes = {}\n",
    "        for adv in self.class_A_adversaries:\n",
    "            self.adversary_classes[adv] = 0  # Class A\n",
    "        for adv in self.class_B_adversaries:\n",
    "            self.adversary_classes[adv] = 1  # Class B\n",
    "\n",
    "    def _get_landmarks(self):\n",
    "        # Assuming landmarks are part of the environment's state\n",
    "        # Placeholder implementation\n",
    "        return []\n",
    "\n",
    "    def get_modified_observation(self, agent_name, original_observation):\n",
    "        if agent_name in self.adversary_classes:\n",
    "            class_id = self.adversary_classes[agent_name]\n",
    "            if class_id == 0:\n",
    "                # Class A adversary observation\n",
    "                obs = self._get_class_A_observation(original_observation)\n",
    "            else:\n",
    "                # Class B adversary observation\n",
    "                obs = self._get_class_B_observation(original_observation)\n",
    "            # Add one-hot class ID\n",
    "            class_id_one_hot = np.zeros(2)\n",
    "            class_id_one_hot[class_id] = 1\n",
    "            obs = np.concatenate([obs, class_id_one_hot])\n",
    "            return obs\n",
    "        else:\n",
    "            # Return original observation for other agents\n",
    "            return original_observation\n",
    "\n",
    "    def _get_class_A_observation(self, original_observation):\n",
    "        # Extract position and velocity of itself\n",
    "        self_pos = original_observation[0:2]\n",
    "        self_vel = original_observation[2:4]\n",
    "        # Absolute positions of landmarks\n",
    "        landmarks_pos = self._get_landmarks_positions()\n",
    "        # Combine into a single observation\n",
    "        obs = np.concatenate([self_pos, self_vel, landmarks_pos])\n",
    "        return obs\n",
    "\n",
    "    def _get_class_B_observation(self, original_observation):\n",
    "        # Extract position and velocity of itself\n",
    "        self_pos = original_observation[0:2]\n",
    "        self_vel = original_observation[2:4]\n",
    "        # Absolute position and velocity of the good agent\n",
    "        good_agent_obs = self._get_good_agent_info()\n",
    "        # Combine into a single observation\n",
    "        obs = np.concatenate([self_pos, self_vel, good_agent_obs])\n",
    "        return obs\n",
    "\n",
    "    def _get_landmarks_positions(self):\n",
    "        # Placeholder implementation\n",
    "        # Return concatenated positions of landmarks\n",
    "        landmarks_pos = []\n",
    "        for lm in self.landmarks:\n",
    "            pos = lm.state.p_pos  # Assuming landmark has position attribute\n",
    "            landmarks_pos.extend(pos)\n",
    "        return np.array(landmarks_pos)\n",
    "\n",
    "    def _get_good_agent_info(self):\n",
    "        # Assuming only one good agent for simplicity\n",
    "        good_agent = self.good_agents[0]\n",
    "        obs = self.env.observe(good_agent)\n",
    "        pos = obs[0:2]\n",
    "        vel = obs[2:4]\n",
    "        return np.concatenate([pos, vel])\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        # Apply actions and get new observations\n",
    "        obs, rewards, dones, infos = self.env.step(action_dict)\n",
    "        modified_obs = {}\n",
    "        for agent_name, original_obs in obs.items():\n",
    "            modified_obs[agent_name] = self.get_modified_observation(agent_name, original_obs)\n",
    "        return modified_obs, rewards, dones, infos\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Communication Mechanism  based on Eucledian distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors_info(adversary_name, adversary_positions, communication_range):\n",
    "    # Get the position of the current adversary\n",
    "    current_pos = adversary_positions[adversary_name]\n",
    "    neighbors = []\n",
    "    for other_name, other_pos in adversary_positions.items():\n",
    "        if other_name != adversary_name:\n",
    "            distance = np.linalg.norm(current_pos - other_pos)\n",
    "            if distance <= communication_range:\n",
    "                neighbors.append(other_name)\n",
    "    return neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3Construction of Graph for Commuincation (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dynamic_graph(adversary_positions, communication_range):\n",
    "    nodes = []\n",
    "    edge_index = [[], []]\n",
    "    node_features = []\n",
    "    node_mapping = {}\n",
    "    \n",
    "    # Assign indices to agents\n",
    "    for idx, (adv_name, pos) in enumerate(adversary_positions.items()):\n",
    "        node_mapping[adv_name] = idx\n",
    "        nodes.append(adv_name)\n",
    "        # Create node features (e.g., processed observations)\n",
    "        feature = adversary_agents[adv_name].temporal_transformer.input_projection(adversary_agents[adv_name].temporal_transformer.input_projection(adversary_agents[adv_name].temporal_transformer.input_projection(processed_observations[adv_name]).detach()))\n",
    "        node_features.append(feature)\n",
    "    \n",
    "    # Determine edges based on communication range\n",
    "    for adv_i in nodes:\n",
    "        idx_i = node_mapping[adv_i]\n",
    "        pos_i = adversary_positions[adv_i]\n",
    "        for adv_j in nodes:\n",
    "            if adv_i != adv_j:\n",
    "                pos_j = adversary_positions[adv_j]\n",
    "                distance = np.linalg.norm(pos_i - pos_j)\n",
    "                if distance <= communication_range:\n",
    "                    idx_j = node_mapping[adv_j]\n",
    "                    # Add edge from adv_i to adv_j\n",
    "                    edge_index[0].append(idx_i)\n",
    "                    edge_index[1].append(idx_j)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    if len(edge_index[0]) == 0:\n",
    "        # No edges, use self-loop to prevent errors\n",
    "        edge_index = torch.tensor([[0], [0]], dtype=torch.long).to(device)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).to(device)\n",
    "    node_features = torch.stack(node_features).to(device)\n",
    "    \n",
    "    return node_features, edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4 Init simple_tag_v2 and wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = simple_tag_v3.env()\n",
    "#env = simple_tag_v3.parallel_env()\n",
    "env.reset()\n",
    "\n",
    "# Define the number of adversaries in each class\n",
    "num_class_A = 2\n",
    "num_class_B = 2\n",
    "\n",
    "# Wrap the environment\n",
    "wrapper = ObservationWrapper(env, num_class_A=num_class_A, num_class_B=num_class_B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Agent Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        action = torch.tanh(self.output_layer(x))  # Assuming continuous action space\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2. Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, state_action):\n",
    "        x = F.relu(self.fc1(state_action))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        value = self.output_layer(x)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 Temporal Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers):\n",
    "        super(TemporalTransformer, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, neighbor_sequences):\n",
    "        # neighbor_sequences shape: (sequence_length, batch_size, input_dim)\n",
    "        x = self.input_projection(neighbor_sequences)\n",
    "        output = self.transformer(x)\n",
    "        return output[-1]  # Return the last output (could be modified as needed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4 GNN Processor (Using GAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNProcessor(nn.Module):\n",
    "    def __init__(self, node_input_dim, hidden_dim, output_dim):\n",
    "        super(GNNProcessor, self).__init__()\n",
    "        self.conv1 = GATConv(node_input_dim, hidden_dim, heads=1)\n",
    "        self.conv2 = GATConv(hidden_dim, output_dim, heads=1)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5 GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNProcessor(nn.Module):\n",
    "    def __init__(self, node_input_dim, hidden_dim, output_dim):\n",
    "        super(GCNProcessor, self).__init__()\n",
    "        self.conv1 = GCNConv(node_input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x: Node features, edge_index: Graph connectivity\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Adversary Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversaryAgent:\n",
    "    def __init__(self, name, class_id, observation_dim, action_dim, hidden_dim, transformer_params, actor, device):\n",
    "        self.name = name\n",
    "        self.class_id = class_id\n",
    "        self.device = device\n",
    "        self.observation_dim = observation_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Actor network (shared among class)\n",
    "        self.actor = actor  # Shared actor passed during initialization\n",
    "\n",
    "        # Temporal Transformer\n",
    "        self.temporal_transformer = TemporalTransformer(\n",
    "            input_dim=observation_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_heads=transformer_params['num_heads'],\n",
    "            num_layers=transformer_params['num_layers']\n",
    "        ).to(device)\n",
    "\n",
    "        # GNN Processor\n",
    "        self.gnn_processor = GNNProcessor(\n",
    "            node_input_dim=hidden_dim * 2,  # Transformer output + own processed obs\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=hidden_dim\n",
    "        ).to(device)\n",
    "\n",
    "        # Optimizers (if needed)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return list(self.temporal_transformer.parameters()) + list(self.gnn_processor.parameters())\n",
    "    \n",
    "    def select_action(self, processed_obs, neighbor_info, edge_index):\n",
    "        # processed_obs: Tensor of own processed observation\n",
    "        # neighbor_info: Tensor of neighbor information from Temporal Transformer\n",
    "        # edge_index: Edge connections for GNN\n",
    "\n",
    "        # Combine own observation and neighbor info\n",
    "        node_features = torch.cat([processed_obs, neighbor_info], dim=-1).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # GNN processing\n",
    "        x = node_features\n",
    "        edge_index = edge_index.to(self.device)\n",
    "        gnn_output = self.gnn_processor(x, edge_index)\n",
    "\n",
    "        # Actor output\n",
    "        action_probs = self.actor(gnn_output)\n",
    "        action_distribution = torch.distributions.Categorical(action_probs)\n",
    "        action = action_distribution.sample()\n",
    "        return action.item(), action_distribution.log_prob(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Init Agents and Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "# Get observation dimension after processing\n",
    "sample_obs = wrapper.env.observe(wrapper.adversaries[0])\n",
    "modified_obs = wrapper.get_modified_observation(wrapper.adversaries[0], sample_obs)\n",
    "observation_dim = len(modified_obs)\n",
    "\n",
    "# Get action dimension\n",
    "action_space = env.action_space(wrapper.adversaries[0])\n",
    "print(f'action_space: {action_space}')\n",
    "\n",
    "action_dim = action_space.n\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_dim = 128\n",
    "transformer_params = {'num_heads': 2, 'num_layers': 2}\n",
    "\n",
    "# Initialize class-specific actors\n",
    "class_actors = {}\n",
    "for class_id in [0, 1]:\n",
    "    actor = ActorNetwork(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=action_dim).to(device)\n",
    "    actor.optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "    class_actors[class_id] = actor\n",
    "\n",
    "# Initialize agents\n",
    "adversary_agents = {}\n",
    "for adv_name in wrapper.adversaries:\n",
    "    class_id = wrapper.adversary_classes[adv_name]\n",
    "    agent = AdversaryAgent(\n",
    "        name=adv_name,\n",
    "        class_id=class_id,\n",
    "        observation_dim=observation_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        transformer_params=transformer_params,\n",
    "        actor=class_actors[class_id],\n",
    "        device=device\n",
    "    )\n",
    "    adversary_agents[adv_name] = agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Init Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_class_based_critic = True\n",
    "\n",
    "# Define global state dimension\n",
    "global_state_dim = observation_dim * len(wrapper.adversaries) + observation_dim * len(wrapper.good_agents)\n",
    "\n",
    "if use_class_based_critic:\n",
    "    class_critics = {}\n",
    "    for class_id in [0, 1]:\n",
    "        critic = CriticNetwork(input_dim=global_state_dim, output_dim=1, hidden_dim=hidden_dim).to(device)\n",
    "        critic.optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "        class_critics[class_id] = critic\n",
    "else:\n",
    "    centralized_critic = CriticNetwork(input_dim=global_state_dim, hidden_dim=hidden_dim).to(device)\n",
    "    centralized_critic.optimizer = optim.Adam(centralized_critic.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for adversary_0, neighbors: ['adversary_1', 'adversary_2'] and their obs: [array([ 0.        ,  0.        , -0.0270679 , -0.8876596 , -0.089927  ,\n",
      "        1.3559995 ,  0.3937757 ,  0.07579712, -0.04470859,  0.3539868 ,\n",
      "        0.92530453,  1.2470239 , -0.32683226,  0.5439994 ,  0.        ,\n",
      "        0.        ], dtype=float32), array([ 0.        ,  0.        ,  0.89823663,  0.35936433, -1.0152315 ,\n",
      "        0.10897546, -0.53152883, -1.1712269 , -0.9700131 , -0.89303714,\n",
      "       -0.92530453, -1.2470239 , -1.2521368 , -0.70302457,  0.        ,\n",
      "        0.        ], dtype=float32)]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [6] at entry 0 and [10] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m     neighbor_sequences\u001b[38;5;241m.\u001b[39mappend(neighbor_obs)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m neighbor_sequences:\n\u001b[0;32m---> 41\u001b[0m     neighbor_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor_sequences\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# (seq_len, batch_size=1, input_dim)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     neighbor_sequences \u001b[38;5;241m=\u001b[39m neighbor_sequences\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Transformer expects (seq_len, batch_size, input_dim)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     neighbor_info \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtemporal_transformer(neighbor_sequences)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [6] at entry 0 and [10] at entry 1"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "max_steps = 100\n",
    "gamma = 0.99\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    wrapper.reset()\n",
    "    observations = {agent: wrapper.env.observe(agent) for agent in wrapper.env.agents}\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    # Initialize episode memory for training\n",
    "    episode_memory = []\n",
    "\n",
    "    while not done and step < max_steps:\n",
    "        actions = {}\n",
    "        log_probs = {}\n",
    "        rewards = {}\n",
    "        # Process observations\n",
    "        processed_observations = {}\n",
    "        adversary_positions = {}\n",
    "        for adv_name, agent in adversary_agents.items():\n",
    "            obs = observations[adv_name]\n",
    "            # Get modified observation\n",
    "            obs = wrapper.get_modified_observation(adv_name, obs)\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "            processed_observations[adv_name] = obs_tensor\n",
    "            # Get position for communication\n",
    "            adversary_positions[adv_name] = obs[0:2]\n",
    "\n",
    "        # Get neighbor information and select actions\n",
    "        for adv_name, agent in adversary_agents.items():\n",
    "            # Get neighbors within communication range\n",
    "            neighbors = get_neighbors_info(adv_name, adversary_positions, communication_range=1.0)\n",
    "            print(f'for {adv_name}, neighbors: {neighbors} and their obs: {[observations[n] for n in neighbors]}')\n",
    "            # Prepare neighbor sequences\n",
    "            neighbor_sequences = []\n",
    "            for neighbor in neighbors:\n",
    "                neighbor_obs = processed_observations[neighbor]\n",
    "                neighbor_sequences.append(neighbor_obs)\n",
    "            if neighbor_sequences:\n",
    "                neighbor_sequences = torch.stack(neighbor_sequences).unsqueeze(1).to(device)  # (seq_len, batch_size=1, input_dim)\n",
    "                neighbor_sequences = neighbor_sequences.transpose(0, 1)  # Transformer expects (seq_len, batch_size, input_dim)\n",
    "                neighbor_info = agent.temporal_transformer(neighbor_sequences)\n",
    "            else:\n",
    "                # If no neighbors, use zeros\n",
    "                neighbor_info = torch.zeros(agent.hidden_dim).to(device)\n",
    "            # Edge index for GNN (self-loop)\n",
    "            edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "            # Select action\n",
    "            action, log_prob = agent.select_action(processed_observations[adv_name], neighbor_info, edge_index)\n",
    "            actions[adv_name] = action\n",
    "            log_probs[adv_name] = log_prob\n",
    "\n",
    "        # Assign random actions to good agents\n",
    "        for good_agent in wrapper.good_agents:\n",
    "            actions[good_agent] = env.action_space(good_agent).sample()\n",
    "\n",
    "        # Step the environment\n",
    "        observations_next, rewards, dones, infos = wrapper.step(actions)\n",
    "        # Store experience\n",
    "        for adv_name, agent in adversary_agents.items():\n",
    "            reward = rewards[adv_name]\n",
    "            done_flag = dones[adv_name]\n",
    "            episode_memory.append({\n",
    "                'agent_name': adv_name,\n",
    "                'log_prob': log_probs[adv_name],\n",
    "                'reward': reward,\n",
    "                'done': done_flag\n",
    "            })\n",
    "\n",
    "        observations = observations_next\n",
    "        done = all(dones.values())\n",
    "        step += 1\n",
    "\n",
    "    # After episode ends, compute returns and update networks\n",
    "    # For simplicity, we'll use REINFORCE algorithm here\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for t in reversed(range(len(episode_memory))):\n",
    "        R = episode_memory[t]['reward'] + gamma * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    # Normalize returns\n",
    "    returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "    # Update actors\n",
    "    policy_loss = {}\n",
    "    for idx, data in enumerate(episode_memory):\n",
    "        adv_name = data['agent_name']\n",
    "        agent = adversary_agents[adv_name]\n",
    "        class_id = agent.class_id\n",
    "        log_prob = data['log_prob']\n",
    "        R = returns[idx]\n",
    "        loss = -log_prob * R\n",
    "        if class_id not in policy_loss:\n",
    "            policy_loss[class_id] = loss\n",
    "        else:\n",
    "            policy_loss[class_id] += loss\n",
    "\n",
    "    for class_id, loss in policy_loss.items():\n",
    "        class_actors[class_id].optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        class_actors[class_id].optimizer.step()\n",
    "\n",
    "    # Update critics if using class-based critic\n",
    "    if use_class_based_critic:\n",
    "        # Prepare global state\n",
    "        global_state = []\n",
    "        for adv_name in wrapper.adversaries:\n",
    "            obs = processed_observations[adv_name].detach().cpu().numpy()\n",
    "            global_state.extend(obs)\n",
    "        for good_agent in wrapper.good_agents:\n",
    "            obs = wrapper.env.observe(good_agent)\n",
    "            global_state.extend(obs)\n",
    "        global_state = torch.tensor(global_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Compute value targets and update critics\n",
    "        for class_id, critic in class_critics.items():\n",
    "            value = critic(global_state)\n",
    "            target = torch.tensor([R], dtype=torch.float32).to(device)\n",
    "            critic_loss = F.mse_loss(value, target)\n",
    "            critic.optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic.optimizer.step()\n",
    "    else:\n",
    "        # Update centralized critic\n",
    "        global_state = []\n",
    "        for adv_name in wrapper.adversaries:\n",
    "            obs = processed_observations[adv_name].detach().cpu().numpy()\n",
    "            global_state.extend(obs)\n",
    "        for good_agent in wrapper.good_agents:\n",
    "            obs = wrapper.env.observe(good_agent)\n",
    "            global_state.extend(obs)\n",
    "        global_state = torch.tensor(global_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        value = centralized_critic(global_state)\n",
    "        target = torch.tensor([R], dtype=torch.float32).to(device)\n",
    "        critic_loss = F.mse_loss(value, target)\n",
    "        centralized_critic.optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        centralized_critic.optimizer.step()\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode} completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "for class_id, actor_info in class_actors.items():\n",
    "    torch.save(actor_info['network'].state_dict(), f'class_{class_id}_actor.pth')\n",
    "\n",
    "#if use_class_based_critic:\n",
    "#    for class_id, critic_info in class_critics.items():\n",
    "#        torch.save(critic_info['network'].state_dict(), f'class_{class_id}_critic.pth')\n",
    "#else:\n",
    "#    torch.save(centralized_critic.state_dict(), 'centralized_critic.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(env, wrapper, adversary_agents, class_actors, class_critics, use_class_based_critic, num_class_A, num_class_B, num_test_episodes=10, max_test_steps=1000):\n",
    "    \"\"\"\n",
    "    Tests the trained model on a different configuration of adversaries.\n",
    "\n",
    "    Parameters:\n",
    "    - env: PettingZoo environment instance.\n",
    "    - wrapper: ObservationWrapper instance.\n",
    "    - adversary_agents: Dictionary of AdversaryAgent instances.\n",
    "    - class_actors: Dictionary of class actors.\n",
    "    - class_critics: Dictionary of class critics.\n",
    "    - use_class_based_critic: Boolean flag for critic type.\n",
    "    - num_class_A: Number of adversaries in Class A for testing.\n",
    "    - num_class_B: Number of adversaries in Class B for testing.\n",
    "    - num_test_episodes: Number of test episodes.\n",
    "    - max_test_steps: Maximum steps per episode.\n",
    "\n",
    "    Returns:\n",
    "    - average_reward: Average reward over test episodes.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(num_test_episodes):\n",
    "        # Reset the environment and wrapper with new class configurations\n",
    "        wrapper.num_class_A = num_class_A\n",
    "        wrapper.num_class_B = num_class_B\n",
    "        modified_obs = wrapper.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done and step < max_test_steps:\n",
    "            actions = {}\n",
    "            # Process observations and select actions for each adversary\n",
    "            adversary_positions = {}\n",
    "            processed_observations = {}\n",
    "            for adv_name, agent in adversary_agents.items():\n",
    "                if adv_name not in wrapper.adversaries:\n",
    "                    continue  # Skip adversaries not present in this test\n",
    "                obs = modified_obs[adv_name]\n",
    "                # Get processed observation\n",
    "                obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "                processed_observations[adv_name] = obs_tensor\n",
    "                # Get position for communication (assuming first two elements are position)\n",
    "                adversary_positions[adv_name] = obs[0:2]\n",
    "\n",
    "            # Communication and neighbor info\n",
    "            neighbor_infos = {}\n",
    "            edge_indices = {}\n",
    "            for adv_name, agent in adversary_agents.items():\n",
    "                if adv_name not in wrapper.adversaries:\n",
    "                    continue  # Skip adversaries not present in this test\n",
    "                # Get neighbors within communication range\n",
    "                neighbors = get_neighbors_info(adv_name, adversary_positions, communication_range=1.0)\n",
    "                # Prepare neighbor sequences\n",
    "                neighbor_sequences = []\n",
    "                for neighbor in neighbors:\n",
    "                    neighbor_obs = processed_observations[neighbor]\n",
    "                    neighbor_sequences.append(neighbor_obs)\n",
    "                if neighbor_sequences:\n",
    "                    neighbor_sequences = torch.stack(neighbor_sequences).unsqueeze(1).to(device)  # (seq_len, batch_size=1, input_dim)\n",
    "                    neighbor_info = agent.temporal_transformer(neighbor_sequences)\n",
    "                else:\n",
    "                    # If no neighbors, use zeros\n",
    "                    neighbor_info = torch.zeros(agent.hidden_dim).to(device)\n",
    "                neighbor_infos[adv_name] = neighbor_info\n",
    "\n",
    "                # Construct dynamic graph\n",
    "                node_features, edge_index = construct_dynamic_graph(adversary_positions, communication_range=1.0)\n",
    "                edge_indices[adv_name] = edge_index\n",
    "\n",
    "            # Select actions\n",
    "            for adv_name, agent in adversary_agents.items():\n",
    "                if adv_name not in wrapper.adversaries:\n",
    "                    continue  # Skip adversaries not present in this test\n",
    "                action, log_prob = agent.select_action(\n",
    "                    processed_observations[adv_name], \n",
    "                    neighbor_infos[adv_name], \n",
    "                    edge_indices[adv_name]\n",
    "                )\n",
    "                actions[adv_name] = action\n",
    "\n",
    "            # Assign random actions to good agents\n",
    "            for good_agent in wrapper.good_agents:\n",
    "                actions[good_agent] = env.action_space(good_agent).sample()\n",
    "\n",
    "            # Step the environment\n",
    "            modified_obs_next, rewards, dones, infos = wrapper.step(actions)\n",
    "\n",
    "            # Accumulate rewards\n",
    "            for adv_name in wrapper.adversaries:\n",
    "                episode_reward += rewards[adv_name]\n",
    "\n",
    "            modified_obs = modified_obs_next\n",
    "            done = all(dones.values())\n",
    "            step += 1\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Test Episode {episode + 1}/{num_test_episodes} Reward: {episode_reward}\")\n",
    "\n",
    "    average_reward = np.mean(total_rewards)\n",
    "    print(f\"Average Reward over {num_test_episodes} Test Episodes: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.Test on different number of agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ObservationWrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m test_env \u001b[38;5;241m=\u001b[39m simple_tag_v2\u001b[38;5;241m.\u001b[39mparallel_env()\n\u001b[1;32m      7\u001b[0m test_env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 8\u001b[0m wrapper_test \u001b[38;5;241m=\u001b[39m \u001b[43mObservationWrapper\u001b[49m(test_env, num_class_A\u001b[38;5;241m=\u001b[39mtest_num_class_A, num_class_B\u001b[38;5;241m=\u001b[39mtest_num_class_B)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load models\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_id, actor \u001b[38;5;129;01min\u001b[39;00m class_actors\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ObservationWrapper' is not defined"
     ]
    }
   ],
   "source": [
    "# To test on different configurations, reinitialize the environment and agents\n",
    "test_num_class_A = 3\n",
    "test_num_class_B = 1\n",
    "\n",
    "# Create new environment and wrapper for testing\n",
    "test_env = simple_tag_v3.parallel_env()\n",
    "test_env.reset()\n",
    "wrapper_test = ObservationWrapper(test_env, num_class_A=test_num_class_A, num_class_B=test_num_class_B)\n",
    "\n",
    "# Load models\n",
    "for class_id, actor in class_actors.items():\n",
    "    actor.load_state_dict(torch.load(f'class_{class_id}_actor.pth'))\n",
    "\n",
    "# Reinitialize agents for testing\n",
    "adversary_agents_test = {}\n",
    "for adv_name in wrapper_test.adversaries:\n",
    "    class_id = wrapper_test.adversary_classes[adv_name]\n",
    "    agent = AdversaryAgent(\n",
    "        name=adv_name,\n",
    "        class_id=class_id,\n",
    "        observation_dim=observation_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        transformer_params=transformer_params,\n",
    "        actor=class_actors[class_id],\n",
    "        device=device\n",
    "    )\n",
    "    adversary_agents_test[adv_name] = agent\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hetnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
