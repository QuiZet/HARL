{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self Velocity: 2 elements (self_vel) \\\n",
    "Self Position: 2 elements (self_pos) \\\n",
    "Positions of Landmarks Relative to Agent: 2 * num_landmarks elements (landmark_rel_positions) \\\n",
    "Positions of Other Agents Relative to Agent: 2 * num_other_agents elements (other_agent_rel_positions)\\\n",
    "Velocities of Other Agents: 2 * num_other_agents elements (other_agent_velocities)\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversary Agents: ['adversary_0', 'adversary_1', 'adversary_2', 'adversary_3']\n",
      "Good Agents: ['agent_0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "import os\n",
    "\n",
    "# Initialize the environment\n",
    "env = simple_tag_v2.parallel_env(render_mode=None, num_adversaries=4, num_good=1, num_obstacles=2)\n",
    "env.reset()\n",
    "\n",
    "# Parameters\n",
    "num_class_a = 3\n",
    "num_class_b = 1\n",
    "num_adversaries = 4\n",
    "num_good_agents = 1  # Only one agent being chased by adversaries\n",
    "num_obstacles = 2\n",
    "\n",
    "# Initial lists of agents\n",
    "adversary_agents = [agent for agent in env.agents if 'adversary' in agent]\n",
    "good_agents = [agent for agent in env.agents if 'agent' in agent]\n",
    "\n",
    "print(\"Adversary Agents:\", adversary_agents)\n",
    "print(\"Good Agents:\", good_agents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Embedding and CGN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src: [batch_size, seq_len, input_dim]\n",
    "        outputs, (hidden, cell) = self.lstm(src)\n",
    "        # hidden, cell: [num_layers, batch_size, hidden_dim]\n",
    "        return hidden, cell\n",
    "\n",
    "# Define Decoder (if needed)\n",
    "# In this case, we might not need a Decoder unless we're generating sequences\n",
    "# For simplicity, let's assume we only need the Encoder's hidden state\n",
    "\n",
    "# Define GCN layer\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.conv1 = pyg_nn.GCNConv(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        return self.conv1(x, edge_index)\n",
    "\n",
    "# Parameters\n",
    "communication_range = 1.5  # Define the communication range\n",
    "input_dim = 8             # Observation dimensions (including class identifier)\n",
    "hidden_dim = 16           # Hidden dimension for LSTM encoder\n",
    "gcn_output_dim = 16       # Dimension of GCN output\n",
    "\n",
    "# Initialize encoder and GCN layer\n",
    "encoder = Encoder(input_dim=input_dim, hidden_dim=hidden_dim)\n",
    "gcn_layer = GCNLayer(input_dim=hidden_dim, output_dim=gcn_output_dim)\n",
    "\n",
    "# Example usage:\n",
    "batch_size = 4\n",
    "input_actor_network_max_dim = 40  # Temporary solution with padding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_decoder_dim = 18\n",
    "#input_actor_network_max_dim = 40 # Temporary solution with padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversary_observation_wrapper(observations, num_class_A, num_class_B, adversary_agents, good_agents, num_adversaries, num_good_agents, num_obstacles):\n",
    "    # Ensure the number of class A and class B agents matches the current number of adversaries\n",
    "    assert num_class_A + num_class_B == len(adversary_agents), \"Number of agents assigned to Class A and Class B must match the total number of adversaries.\"\n",
    "    \n",
    "    class_a_agents = adversary_agents[:num_class_A]\n",
    "    class_b_agents = adversary_agents[num_class_A:num_class_A + num_class_B]\n",
    "    \n",
    "    updated_observations = {}\n",
    "    adversary_positions = {}  # Store adversary positions for communication\n",
    "    \n",
    "    num_landmarks = num_obstacles  # In simple_tag_v2, obstacles are considered as landmarks\n",
    "    num_other_agents = num_good_agents + num_adversaries - 1  # Excluding self\n",
    "\n",
    "    # Indices for observation components\n",
    "    self_vel_indices = slice(0, 2)\n",
    "    self_pos_indices = slice(2, 4)\n",
    "    landmark_indices = slice(4, 4 + 2 * num_landmarks)\n",
    "    other_agent_rel_pos_indices = slice(4 + 2 * num_landmarks, 4 + 2 * num_landmarks + 2 * num_other_agents)\n",
    "    other_agent_vel_indices = slice(4 + 2 * num_landmarks + 2 * num_other_agents, 4 + 2 * num_landmarks + 4 * num_other_agents)\n",
    "\n",
    "    for agent, obs in observations.items():\n",
    "        if agent in adversary_agents:\n",
    "            # Add class identifier\n",
    "            agent_class = 0 if agent in class_a_agents else 1  # Class A: 0, Class B: 1\n",
    "\n",
    "            # Extract self_vel and self_pos\n",
    "            self_vel = obs[self_vel_indices]  # indices 0-1\n",
    "            self_pos = obs[self_pos_indices]  # indices 2-3\n",
    "\n",
    "            if agent in class_a_agents:\n",
    "                # For Class A adversaries: self_vel, self_pos, landmark positions\n",
    "                landmark_rel_positions = obs[landmark_indices]\n",
    "                updated_obs = np.concatenate([self_vel, self_pos, landmark_rel_positions, [agent_class]])\n",
    "            elif agent in class_b_agents:\n",
    "                # For Class B adversaries: self_vel, self_pos, position and velocity of the good agent\n",
    "                # Assuming the good agent is the first in the list of other agents\n",
    "                good_agent_rel_pos = obs[other_agent_rel_pos_indices][0:2]\n",
    "                good_agent_vel = obs[other_agent_vel_indices][0:2]\n",
    "                updated_obs = np.concatenate([self_vel, self_pos, good_agent_rel_pos, good_agent_vel, [agent_class]])\n",
    "            else:\n",
    "                # Should not reach here\n",
    "                updated_obs = obs  # Keep as is\n",
    "            updated_observations[agent] = updated_obs\n",
    "        elif agent in good_agents:\n",
    "            updated_observations[agent] = obs  # Good agents keep their observation\n",
    "        else:\n",
    "            # Handle any unexpected agents\n",
    "            updated_observations[agent] = obs\n",
    "    \n",
    "    # Step 2: Gather positions for communication (using updated observations)\n",
    "    for agent in adversary_agents:\n",
    "        position = updated_observations[agent][2:4]  # Self position is at indices 2-3 after truncation\n",
    "        adversary_positions[agent] = position\n",
    "    \n",
    "    # Step 3: Communication and gather data for GCN input\n",
    "    node_features = {}\n",
    "    edge_index = []\n",
    "    agent_to_idx = {agent: idx for idx, agent in enumerate(adversary_agents)}\n",
    "    \n",
    "    for agent in adversary_agents:\n",
    "        own_position = adversary_positions[agent]\n",
    "        neighbors = []\n",
    "        \n",
    "        # Find neighbors within communication range\n",
    "        for other_agent in adversary_agents:\n",
    "            if agent != other_agent:\n",
    "                other_position = adversary_positions[other_agent]\n",
    "                distance = np.linalg.norm(own_position - other_position)\n",
    "                if distance <= communication_range:\n",
    "                    neighbors.append(other_agent)\n",
    "                    edge_index.append([agent_to_idx[agent], agent_to_idx[other_agent]])  # Add edge\n",
    "        \n",
    "        # Step 4: Collect observations of neighbors to form a sequence\n",
    "        neighbor_obs_list = []\n",
    "        for neighbor in neighbors:\n",
    "            neighbor_obs = updated_observations[neighbor][:-1]  # Exclude class identifier\n",
    "            neighbor_obs_list.append(neighbor_obs)\n",
    "        \n",
    "        # If no neighbors, use a zero tensor\n",
    "        if len(neighbor_obs_list) == 0:\n",
    "            neighbor_obs_seq = torch.zeros(1, 1, len(updated_observations[agent]) - 1)  # [batch_size, seq_len, input_dim]\n",
    "        else:\n",
    "            neighbor_obs_seq = torch.tensor(neighbor_obs_list, dtype=torch.float32)\n",
    "            neighbor_obs_seq = neighbor_obs_seq.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Step 5: Pass the sequence through the Encoder\n",
    "        hidden, cell = encoder(neighbor_obs_seq)\n",
    "        # Use the last layer's hidden state\n",
    "        encoder_output = hidden[-1, :, :]  # [batch_size, hidden_dim]\n",
    "        node_features[agent] = encoder_output.squeeze(0)  # Remove batch dimension\n",
    "    \n",
    "    # Convert node_features to a list in the order of agents\n",
    "    node_features_list = [node_features[agent] for agent in adversary_agents]\n",
    "    node_features_tensor = torch.stack(node_features_list)  # [num_nodes, hidden_dim]\n",
    "    \n",
    "    # Convert edge_index to tensor\n",
    "    if edge_index:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    else:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    \n",
    "    # Step 6: Pass the gathered information into a GCN\n",
    "    gcn_output = gcn_layer(node_features_tensor, edge_index)\n",
    "    \n",
    "    # Step 7: Concatenate GCN output with agent's own observation\n",
    "    final_inputs = {}\n",
    "    for agent_idx, agent in enumerate(adversary_agents):\n",
    "        own_obs = torch.tensor(updated_observations[agent][:-1], dtype=torch.float32)  # Exclude class identifier\n",
    "        class_id = torch.tensor([updated_observations[agent][-1]], dtype=torch.float32)\n",
    "        final_input = torch.cat([own_obs, class_id, gcn_output[agent_idx]])\n",
    "        final_inputs[agent] = final_input  # This will be used as input to the Actor network\n",
    "    \n",
    "    return updated_observations, final_inputs  # Return the updated observations and inputs for Actor network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Environment and Run a Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Observations:\n",
      "adversary_0: [ 3.53278325e-35  3.00000012e-01 -3.23127896e-01  5.91029465e-01\n",
      " -1.51076391e-01  2.84479797e-01  6.52115047e-02 -5.66242814e-01\n",
      "  0.00000000e+00]\n",
      "adversary_1: [-0.         -0.         -0.6514588  -0.27028221  0.17725453  1.14579153\n",
      "  0.39354241  0.29506889  0.        ]\n",
      "adversary_2: [ 0.30000001 -0.          0.03634214 -0.8105517  -0.51054645  1.68606102\n",
      " -0.29425853  0.83533841  0.        ]\n",
      "adversary_3: [ 7.10177243e-01 -7.04022944e-01 -3.36809784e-01  7.52637565e-01\n",
      "  1.36818849e-02 -1.61608130e-01 -4.13254473e-13  4.56688832e-13\n",
      "  1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment\n",
    "env.reset()\n",
    "\n",
    "# Sample action spaces for all agents\n",
    "actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "# Step through the environment\n",
    "observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "# Update adversary_agents and good_agents based on current observations\n",
    "adversary_agents = [agent for agent in observations.keys() if 'adversary' in agent]\n",
    "good_agents = [agent for agent in observations.keys() if 'agent' in agent]\n",
    "\n",
    "# Apply the observation wrapper for adversaries\n",
    "observations, final_inputs = adversary_observation_wrapper(\n",
    "    observations, num_class_a, num_class_b, adversary_agents, good_agents, num_adversaries, num_good_agents, num_obstacles)\n",
    "\n",
    "print(\"Updated Observations:\")\n",
    "for agent in adversary_agents:\n",
    "    print(f\"{agent}: {observations[agent]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: adversary_0, Input Dim: 35, Output Dim: 5\n",
      "Agent: adversary_1, Input Dim: 35, Output Dim: 5\n",
      "Agent: adversary_2, Input Dim: 35, Output Dim: 5\n",
      "Agent: adversary_3, Input Dim: 35, Output Dim: 5\n"
     ]
    }
   ],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, output_dim)  # Output dimension should match the action space\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize Actor networks for adversaries\n",
    "actor_networks = {}\n",
    "for agent in adversary_agents:\n",
    "    input_dim = len(env.observation_space(agent).low) + 1 + gcn_output_dim  # Observation length plus class id plus GCN output\n",
    "    output_dim = env.action_space(agent).n  # Assuming discrete action space\n",
    "    print(f'Agent: {agent}, Input Dim: {input_dim}, Output Dim: {output_dim}')\n",
    "    input_dim = max(input_dim, input_actor_network_max_dim)\n",
    "    actor_networks[agent] = ActorNetwork(input_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yungisimon/anaconda3/envs/harl/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10: Average Reward: -14.011727975773152\n",
      "Episode 20: Average Reward: 15.453780121149288\n",
      "Episode 30: Average Reward: 29.0997756679095\n",
      "Episode 40: Average Reward: -20.820313140910805\n",
      "Episode 50: Average Reward: -0.7240371551667192\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_episodes = 50  # Total number of episodes to run\n",
    "print_interval = 10  # Print rewards every 10 episodes\n",
    "\n",
    "# Initialize reward tracking\n",
    "episode_rewards = []\n",
    "\n",
    "# Optimizers for Actor networks and GCN (assuming we are training them)\n",
    "learning_rate = 0.001\n",
    "actor_optimizers = {agent: torch.optim.Adam(actor_networks[agent].parameters(), lr=learning_rate) for agent in adversary_agents}\n",
    "gcn_optimizer = torch.optim.Adam(gcn_layer.parameters(), lr=learning_rate)\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss function (placeholder, you need to define based on your RL algorithm)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    observations = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0  # Reset cumulative reward for the episode\n",
    "    \n",
    "    while not done:\n",
    "        # Update adversary_agents and good_agents based on current observations\n",
    "        adversary_agents = [agent for agent in observations.keys() if 'adversary' in agent]\n",
    "        good_agents = [agent for agent in observations.keys() if 'agent' in agent]\n",
    "        \n",
    "        num_adversaries = len(adversary_agents)\n",
    "        num_good_agents = len(good_agents)\n",
    "        \n",
    "        # Apply the observation wrapper for adversaries\n",
    "        observations, final_inputs = adversary_observation_wrapper(\n",
    "            observations, num_class_a, num_class_b, adversary_agents, good_agents, num_adversaries, num_good_agents, num_obstacles)\n",
    "        \n",
    "        actions = {}\n",
    "        for agent in env.agents:\n",
    "            if agent in adversary_agents:\n",
    "                # Pad the input to match the maximum input dimension\n",
    "                m = nn.ConstantPad1d((0, input_actor_network_max_dim - final_inputs[agent].shape[0]), 0)\n",
    "                final_inputs_pad = m(final_inputs[agent])\n",
    "                \n",
    "                # Get the input for the Actor network\n",
    "                actor_input = final_inputs_pad\n",
    "                # Get action probabilities (assuming discrete action space)\n",
    "                action_probs = actor_networks[agent](actor_input)\n",
    "                # Sample an action (for simplicity, we take the action with the highest probability)\n",
    "                action = torch.argmax(action_probs).item()\n",
    "                actions[agent] = action\n",
    "            elif agent in good_agents:\n",
    "                # For good agents, sample random actions\n",
    "                actions[agent] = env.action_space(agent).sample()\n",
    "            else:\n",
    "                # Handle any unexpected agents\n",
    "                actions[agent] = env.action_space(agent).sample()\n",
    "        \n",
    "        # Step the environment\n",
    "        next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "        \n",
    "        # Update cumulative reward\n",
    "        cumulative_reward += sum(rewards.values())\n",
    "        \n",
    "        # Placeholder for training step (you need to implement your RL algorithm here)\n",
    "        # For example, compute loss and update networks\n",
    "        \n",
    "        # For simplicity, let's assume we have a target value (dummy value here)\n",
    "        target = torch.zeros(1)\n",
    "        loss = 0\n",
    "        for agent in adversary_agents:\n",
    "            # Pad the input to match the maximum input dimension\n",
    "            m = nn.ConstantPad1d((0, input_actor_network_max_dim - final_inputs[agent].shape[0]), 0)\n",
    "            final_inputs_pad = m(final_inputs[agent])\n",
    "            \n",
    "            # Get the predicted value\n",
    "            actor_input = final_inputs_pad\n",
    "            prediction = actor_networks[agent](actor_input)\n",
    "            # Compute loss (this is a placeholder)\n",
    "            loss += loss_fn(prediction.unsqueeze(0), target)\n",
    "        \n",
    "        # Backpropagation\n",
    "        encoder_optimizer.zero_grad()\n",
    "        gcn_optimizer.zero_grad()\n",
    "        for optimizer in actor_optimizers.values():\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        encoder_optimizer.step()\n",
    "        gcn_optimizer.step()\n",
    "        for optimizer in actor_optimizers.values():\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Update observations\n",
    "        observations = next_observations\n",
    "        \n",
    "        # Check if all agents are done\n",
    "        done = all(terminations.values()) or all(truncations.values())\n",
    "    \n",
    "    # Append cumulative reward for the episode\n",
    "    episode_rewards.append(cumulative_reward)\n",
    "    \n",
    "    # Print rewards every 'print_interval' episodes\n",
    "    if episode % print_interval == 0:\n",
    "        avg_reward = sum(episode_rewards[-print_interval:]) / print_interval\n",
    "        print(f\"Episode {episode}: Average Reward: {avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a directory to save models\n",
    "model_dir = 'saved_models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "\n",
    "def save_models(num_class_A, num_class_B, adversary_agents):\n",
    "    class_a_agents = adversary_agents[:num_class_A]\n",
    "    class_b_agents = adversary_agents[num_class_A:num_class_A + num_class_B]\n",
    "    for agent in class_a_agents:\n",
    "        torch.save(actor_networks[agent].state_dict(), os.path.join(model_dir, f\"actor_class_A.pth\"))\n",
    "        break  # Save only one model for Class A (assuming they share weights)\n",
    "    for agent in class_b_agents:\n",
    "        torch.save(actor_networks[agent].state_dict(), os.path.join(model_dir, f\"actor_class_B.pth\"))\n",
    "        break  # Save only one model for Class B (assuming they share weights)\n",
    "\n",
    "save_models(num_class_a, num_class_b, adversary_agents)\n",
    "# Save the GCN and Encoder models\n",
    "torch.save(gcn_layer.state_dict(), os.path.join(model_dir, \"gcn_model.pth\"))\n",
    "torch.save(encoder.state_dict(), os.path.join(model_dir, \"encoder_model.pth\"))\n",
    "\n",
    "print(\"Models saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Models and Test with Different Number of Adversaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Encoder:\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([64, 5]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the models\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Define Encoder and GCN layer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Encoder(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim)  \u001b[38;5;66;03m# Adjust input_dim as needed\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m gcn_layer \u001b[38;5;241m=\u001b[39m GCNLayer(input_dim\u001b[38;5;241m=\u001b[39mhidden_dim, output_dim\u001b[38;5;241m=\u001b[39mgcn_output_dim)\n\u001b[1;32m      7\u001b[0m gcn_layer\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgcn_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/envs/harl/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Encoder:\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([64, 8]) from checkpoint, the shape in current model is torch.Size([64, 5])."
     ]
    }
   ],
   "source": [
    "# Load the models\n",
    "# Define Encoder and GCN layer\n",
    "encoder = Encoder(input_dim=5, hidden_dim=hidden_dim)  # Adjust input_dim as needed\n",
    "encoder.load_state_dict(torch.load(os.path.join(model_dir, \"encoder_model.pth\")))\n",
    "\n",
    "gcn_layer = GCNLayer(input_dim=hidden_dim, output_dim=gcn_output_dim)\n",
    "gcn_layer.load_state_dict(torch.load(os.path.join(model_dir, \"gcn_model.pth\")))\n",
    "\n",
    "# Re-initialize Actor networks for new agents and load the saved models\n",
    "actor_networks = {}\n",
    "\n",
    "# You might test with a different number of adversaries here\n",
    "# For example, testing with 5 adversaries\n",
    "test_num_adversaries = 5\n",
    "test_num_class_a = 3\n",
    "test_num_class_b = 2\n",
    "\n",
    "# Initialize the environment with the new number of adversaries\n",
    "env = simple_tag_v2.parallel_env(render_mode=None, num_adversaries=test_num_adversaries, num_good=1, num_obstacles=2)\n",
    "env.reset()\n",
    "\n",
    "# Get the new lists of adversaries and good agents\n",
    "adversary_agents = [agent for agent in env.agents if 'adversary' in agent]\n",
    "good_agents = [agent for agent in env.agents if 'agent' in agent]\n",
    "\n",
    "# Initialize Actor networks for the new adversaries\n",
    "class_a_agents = adversary_agents[:test_num_class_a]\n",
    "class_b_agents = adversary_agents[test_num_class_a:test_num_class_a + test_num_class_b]\n",
    "\n",
    "for agent in adversary_agents:\n",
    "    input_dim = input_actor_network_max_dim  # Use the same padded input dim as before\n",
    "    output_dim = env.action_space(agent).n  # Assuming discrete action space\n",
    "    actor_net = ActorNetwork(input_dim, output_dim)\n",
    "    \n",
    "    # Load the appropriate saved model based on the class of the agent\n",
    "    if agent in class_a_agents:\n",
    "        actor_net.load_state_dict(torch.load(os.path.join(model_dir, f\"actor_class_A.pth\")))\n",
    "    else:\n",
    "        actor_net.load_state_dict(torch.load(os.path.join(model_dir, f\"actor_class_B.pth\")))\n",
    "    \n",
    "    actor_networks[agent] = actor_net\n",
    "\n",
    "# Run the environment for testing\n",
    "num_epochs = 50\n",
    "print_interval = 10  # Calculate average reward every 10 epochs\n",
    "episode_rewards = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    cumulative_reward = 0  # Reset cumulative reward for the epoch\n",
    "    observations = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Update adversary_agents and good_agents based on current observations\n",
    "        adversary_agents = [agent for agent in observations.keys() if 'adversary' in agent]\n",
    "        good_agents = [agent for agent in observations.keys() if 'agent' in agent]\n",
    "        \n",
    "        num_adversaries = len(adversary_agents)\n",
    "        num_good_agents = len(good_agents)\n",
    "        \n",
    "        # Apply the observation wrapper for adversaries\n",
    "        observations, final_inputs = adversary_observation_wrapper(\n",
    "            observations, test_num_class_a, test_num_class_b, adversary_agents, good_agents, num_adversaries, num_good_agents, num_obstacles\n",
    "        )\n",
    "\n",
    "        actions = {}\n",
    "        for agent in adversary_agents:\n",
    "            # Pad the input to match the maximum input dimension\n",
    "            m = nn.ConstantPad1d((0, input_actor_network_max_dim - final_inputs[agent].shape[0]), 0)\n",
    "            final_inputs_pad = m(final_inputs[agent])\n",
    "            \n",
    "            # Pass through the Actor network\n",
    "            action_probs = actor_networks[agent](final_inputs_pad)\n",
    "            action = torch.argmax(action_probs).item()  # Take the action with the highest probability\n",
    "            actions[agent] = action\n",
    "\n",
    "        # For good agents, sample random actions\n",
    "        for agent in good_agents:\n",
    "            actions[agent] = env.action_space(agent).sample()\n",
    "\n",
    "        # Step the environment\n",
    "        next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        # Update cumulative reward for this epoch\n",
    "        cumulative_reward += sum(rewards.values())\n",
    "\n",
    "        # Check if all agents are done\n",
    "        if all(terminations.values()) or all(truncations.values()):\n",
    "            done = True\n",
    "\n",
    "        # Update observations\n",
    "        observations = next_observations\n",
    "\n",
    "    # Track the cumulative reward for the epoch\n",
    "    episode_rewards.append(cumulative_reward)\n",
    "\n",
    "    # Print average reward every 'print_interval' epochs\n",
    "    if epoch % print_interval == 0:\n",
    "        avg_reward = sum(episode_rewards[-print_interval:]) / print_interval\n",
    "        print(f\"Epoch {epoch}: Average Reward over last {print_interval} epochs: {avg_reward}\")\n",
    "\n",
    "print(\"Test completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
