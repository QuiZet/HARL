{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversary Agents: ['adversary_0', 'adversary_1', 'adversary_2', 'adversary_3']\n",
      "Good Agents: ['agent_0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "import os\n",
    "\n",
    "# Initialize the environment\n",
    "env = simple_tag_v2.parallel_env(render_mode=None, num_adversaries=4, num_good=1, num_obstacles=2)\n",
    "env.reset()\n",
    "\n",
    "# Parameters\n",
    "num_class_a = 3\n",
    "num_class_b = 1\n",
    "num_adversaries = 4\n",
    "num_good_agents = 1  # Only one agent being chased by adversaries\n",
    "num_obstacles = 2\n",
    "\n",
    "# Initial lists of agents\n",
    "adversary_agents = [agent for agent in env.agents if 'adversary' in agent]\n",
    "good_agents = [agent for agent in env.agents if 'agent' in agent]\n",
    "\n",
    "print(\"Adversary Agents:\", adversary_agents)\n",
    "print(\"Good Agents:\", good_agents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Embedding and CGN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src: [batch_size, seq_len, input_dim]\n",
    "        outputs, (hidden, cell) = self.lstm(src)\n",
    "        # hidden, cell: [num_layers, batch_size, hidden_dim]\n",
    "        return hidden, cell\n",
    "\n",
    "# Define Decoder (if needed)\n",
    "# In this case, we might not need a Decoder unless we're generating sequences\n",
    "# For simplicity, let's assume we only need the Encoder's hidden state\n",
    "\n",
    "# Define GCN layer\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.conv1 = pyg_nn.GCNConv(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        return self.conv1(x, edge_index)\n",
    "\n",
    "# Parameters\n",
    "communication_range = 1.5  # Define the communication range\n",
    "input_dim = 5             # Observation dimensions (including class identifier)\n",
    "hidden_dim = 16           # Hidden dimension for LSTM encoder\n",
    "gcn_output_dim = 16       # Dimension of GCN output\n",
    "\n",
    "# Initialize encoder and GCN layer\n",
    "encoder = Encoder(input_dim=input_dim, hidden_dim=hidden_dim)\n",
    "gcn_layer = GCNLayer(input_dim=hidden_dim, output_dim=gcn_output_dim)\n",
    "\n",
    "# Example usage:\n",
    "batch_size = 4\n",
    "input_actor_network_max_dim = 40  # Temporary solution with padding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_decoder_dim = 18\n",
    "#input_actor_network_max_dim = 40 # Temporary solution with padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversary_observation_wrapper(observations, num_class_A, num_class_B, adversary_agents, good_agents, num_adversaries, num_good_agents, num_obstacles):\n",
    "    # Ensure the number of class A and class B agents matches the current number of adversaries\n",
    "    assert num_class_A + num_class_B == len(adversary_agents), \"Number of agents assigned to Class A and Class B must match the total number of adversaries.\"\n",
    "    \n",
    "    class_a_agents = adversary_agents[:num_class_A]\n",
    "    class_b_agents = adversary_agents[num_class_A:num_class_A + num_class_B]\n",
    "    \n",
    "    updated_observations = {}\n",
    "    adversary_positions = {}  # Store adversary positions for communication\n",
    "    \n",
    "    # Step 1: Modify observations to include class identifier\n",
    "    for agent, obs in observations.items():\n",
    "        if agent in adversary_agents:\n",
    "            agent_class = 0 if agent in class_a_agents else 1  # Class A: 0, Class B: 1\n",
    "            updated_obs = np.concatenate([obs, [agent_class]])  # Add class identifier\n",
    "            updated_observations[agent] = updated_obs\n",
    "        elif agent in good_agents:\n",
    "            updated_observations[agent] = obs  # Good agents keep their observation\n",
    "        else:\n",
    "            # Handle any unexpected agents\n",
    "            updated_observations[agent] = obs\n",
    "    \n",
    "    # Step 2: Gather positions for communication\n",
    "    for agent in adversary_agents:\n",
    "        position = updated_observations[agent][2:4]  # Assume position is at index 2:4\n",
    "        adversary_positions[agent] = position\n",
    "    \n",
    "    # Step 3: Communication and gather data for GCN input\n",
    "    node_features = {}\n",
    "    edge_index = []\n",
    "    agent_to_idx = {agent: idx for idx, agent in enumerate(adversary_agents)}\n",
    "    \n",
    "    for agent in adversary_agents:\n",
    "        own_position = adversary_positions[agent]\n",
    "        neighbors = []\n",
    "        \n",
    "        # Find neighbors within communication range\n",
    "        for other_agent in adversary_agents:\n",
    "            if agent != other_agent:\n",
    "                other_position = adversary_positions[other_agent]\n",
    "                distance = np.linalg.norm(own_position - other_position)\n",
    "                if distance <= communication_range:\n",
    "                    neighbors.append(other_agent)\n",
    "                    edge_index.append([agent_to_idx[agent], agent_to_idx[other_agent]])  # Add edge\n",
    "        \n",
    "        # Step 4: Collect observations of neighbors to form a sequence\n",
    "        neighbor_obs_list = []\n",
    "        for neighbor in neighbors:\n",
    "            neighbor_obs = updated_observations[neighbor][:5]  # Exclude class identifier\n",
    "            neighbor_obs_list.append(neighbor_obs)\n",
    "        \n",
    "        # If no neighbors, use a zero tensor\n",
    "        if len(neighbor_obs_list) == 0:\n",
    "            neighbor_obs_seq = torch.zeros(1, 1, input_dim)  # [batch_size, seq_len, input_dim]\n",
    "        else:\n",
    "            neighbor_obs_seq = torch.tensor(neighbor_obs_list, dtype=torch.float32)\n",
    "            neighbor_obs_seq = neighbor_obs_seq.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Step 5: Pass the sequence through the Encoder\n",
    "        hidden, cell = encoder(neighbor_obs_seq)\n",
    "        # Use the last layer's hidden state\n",
    "        encoder_output = hidden[-1, :, :]  # [batch_size, hidden_dim]\n",
    "        node_features[agent] = encoder_output.squeeze(0)  # Remove batch dimension\n",
    "    \n",
    "    # Convert node_features to a list in the order of agents\n",
    "    node_features_list = [node_features[agent] for agent in adversary_agents]\n",
    "    node_features_tensor = torch.stack(node_features_list)  # [num_nodes, hidden_dim]\n",
    "    \n",
    "    # Convert edge_index to tensor\n",
    "    if edge_index:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    else:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    \n",
    "    # Step 6: Pass the gathered information into a GCN\n",
    "    gcn_output = gcn_layer(node_features_tensor, edge_index)\n",
    "    \n",
    "    # Step 7: Concatenate GCN output with agent's own observation\n",
    "    final_inputs = {}\n",
    "    for agent_idx, agent in enumerate(adversary_agents):\n",
    "        own_obs = torch.tensor(updated_observations[agent], dtype=torch.float32)\n",
    "        final_input = torch.cat([own_obs, gcn_output[agent_idx]])\n",
    "        final_inputs[agent] = final_input  # This will be used as input to the Actor network\n",
    "    \n",
    "    return updated_observations, final_inputs  # Return the updated observations and inputs for Actor network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Environment and Run a Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Observations:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107126/822196652.py:56: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  neighbor_obs_seq = torch.tensor(neighbor_obs_list, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment\n",
    "env.reset()\n",
    "\n",
    "# Sample action spaces for all agents\n",
    "actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "# Step through the environment\n",
    "observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "# Update adversary_agents and good_agents based on current observations\n",
    "adversary_agents = [agent for agent in observations.keys() if 'adversary' in agent]\n",
    "good_agents = [agent for agent in observations.keys() if 'agent' in agent]\n",
    "\n",
    "# Apply the observation wrapper for adversaries\n",
    "observations, final_inputs = adversary_observation_wrapper(\n",
    "    observations, num_class_a, num_class_b, adversary_agents, good_agents, num_adversaries, num_good_agents, num_obstacles)\n",
    "\n",
    "print(\"Updated Observations:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: adversary_0, Input Dim: 35, Output Dim: 5\n",
      "Agent: adversary_1, Input Dim: 35, Output Dim: 5\n",
      "Agent: adversary_2, Input Dim: 35, Output Dim: 5\n",
      "Agent: adversary_3, Input Dim: 35, Output Dim: 5\n"
     ]
    }
   ],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, output_dim)  # Output dimension should match the action space\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize Actor networks for adversaries\n",
    "actor_networks = {}\n",
    "for agent in adversary_agents:\n",
    "    input_dim = len(env.observation_space(agent).low) + 1 + gcn_output_dim  # Observation length plus class id plus GCN output\n",
    "    output_dim = env.action_space(agent).n  # Assuming discrete action space\n",
    "    print(f'Agent: {agent}, Input Dim: {input_dim}, Output Dim: {output_dim}')\n",
    "    input_dim = max(input_dim, input_actor_network_max_dim)\n",
    "    actor_networks[agent] = ActorNetwork(input_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yungisimon/anaconda3/envs/harl/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 5, got 40",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m num_good_agents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(good_agents)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Apply the observation wrapper for adversaries\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m observations, final_inputs \u001b[38;5;241m=\u001b[39m \u001b[43madversary_observation_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_class_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_class_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madversary_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgood_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_adversaries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_good_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_obstacles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m actions \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m env\u001b[38;5;241m.\u001b[39magents:\n",
      "Cell \u001b[0;32mIn[4], line 60\u001b[0m, in \u001b[0;36madversary_observation_wrapper\u001b[0;34m(observations, num_class_A, num_class_B, adversary_agents, good_agents, num_adversaries, num_good_agents, num_obstacles)\u001b[0m\n\u001b[1;32m     57\u001b[0m     neighbor_obs_seq \u001b[38;5;241m=\u001b[39m neighbor_obs_seq\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Step 5: Pass the sequence through the Encoder\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m hidden, cell \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor_obs_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Use the last layer's hidden state\u001b[39;00m\n\u001b[1;32m     62\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, :]  \u001b[38;5;66;03m# [batch_size, hidden_dim]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/harl/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# src: [batch_size, seq_len, input_dim]\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     outputs, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# hidden, cell: [num_layers, batch_size, hidden_dim]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden, cell\n",
      "File \u001b[0;32m~/anaconda3/envs/harl/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/harl/lib/python3.10/site-packages/torch/nn/modules/rnn.py:810\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 810\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    812\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m~/anaconda3/envs/harl/lib/python3.10/site-packages/torch/nn/modules/rnn.py:730\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    726\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    727\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    728\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    729\u001b[0m                        ):\n\u001b[0;32m--> 730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    732\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    734\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/harl/lib/python3.10/site-packages/torch/nn/modules/rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    216\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    220\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 5, got 40"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_episodes = 50  # Total number of episodes to run\n",
    "print_interval = 10  # Print rewards every 10 episodes\n",
    "\n",
    "# Initialize reward tracking\n",
    "episode_rewards = []\n",
    "\n",
    "# Optimizers for Actor networks and GCN (assuming we are training them)\n",
    "learning_rate = 0.001\n",
    "actor_optimizers = {agent: torch.optim.Adam(actor_networks[agent].parameters(), lr=learning_rate) for agent in adversary_agents}\n",
    "gcn_optimizer = torch.optim.Adam(gcn_layer.parameters(), lr=learning_rate)\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss function (placeholder, you need to define based on your RL algorithm)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    observations = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0  # Reset cumulative reward for the episode\n",
    "    \n",
    "    while not done:\n",
    "        # Update adversary_agents and good_agents based on current observations\n",
    "        adversary_agents = [agent for agent in observations.keys() if 'adversary' in agent]\n",
    "        good_agents = [agent for agent in observations.keys() if 'agent' in agent]\n",
    "        \n",
    "        num_adversaries = len(adversary_agents)\n",
    "        num_good_agents = len(good_agents)\n",
    "        \n",
    "        # Apply the observation wrapper for adversaries\n",
    "        observations, final_inputs = adversary_observation_wrapper(\n",
    "            observations, num_class_a, num_class_b, adversary_agents, good_agents, num_adversaries, num_good_agents, num_obstacles)\n",
    "        \n",
    "        actions = {}\n",
    "        for agent in env.agents:\n",
    "            if agent in adversary_agents:\n",
    "                # Pad the input to match the maximum input dimension\n",
    "                m = nn.ConstantPad1d((0, input_actor_network_max_dim - final_inputs[agent].shape[0]), 0)\n",
    "                final_inputs_pad = m(final_inputs[agent])\n",
    "                \n",
    "                # Get the input for the Actor network\n",
    "                actor_input = final_inputs_pad\n",
    "                # Get action probabilities (assuming discrete action space)\n",
    "                action_probs = actor_networks[agent](actor_input)\n",
    "                # Sample an action (for simplicity, we take the action with the highest probability)\n",
    "                action = torch.argmax(action_probs).item()\n",
    "                actions[agent] = action\n",
    "            elif agent in good_agents:\n",
    "                # For good agents, sample random actions\n",
    "                actions[agent] = env.action_space(agent).sample()\n",
    "            else:\n",
    "                # Handle any unexpected agents\n",
    "                actions[agent] = env.action_space(agent).sample()\n",
    "        \n",
    "        # Step the environment\n",
    "        next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "        \n",
    "        # Update cumulative reward\n",
    "        cumulative_reward += sum(rewards.values())\n",
    "        \n",
    "        # Placeholder for training step (you need to implement your RL algorithm here)\n",
    "        # For example, compute loss and update networks\n",
    "        \n",
    "        # For simplicity, let's assume we have a target value (dummy value here)\n",
    "        target = torch.zeros(1)\n",
    "        loss = 0\n",
    "        for agent in adversary_agents:\n",
    "            # Pad the input to match the maximum input dimension\n",
    "            m = nn.ConstantPad1d((0, input_actor_network_max_dim - final_inputs[agent].shape[0]), 0)\n",
    "            final_inputs_pad = m(final_inputs[agent])\n",
    "            \n",
    "            # Get the predicted value\n",
    "            actor_input = final_inputs_pad\n",
    "            prediction = actor_networks[agent](actor_input)\n",
    "            # Compute loss (this is a placeholder)\n",
    "            loss += loss_fn(prediction.unsqueeze(0), target)\n",
    "        \n",
    "        # Backpropagation\n",
    "        encoder_optimizer.zero_grad()\n",
    "        gcn_optimizer.zero_grad()\n",
    "        for optimizer in actor_optimizers.values():\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        encoder_optimizer.step()\n",
    "        gcn_optimizer.step()\n",
    "        for optimizer in actor_optimizers.values():\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Update observations\n",
    "        observations = next_observations\n",
    "        \n",
    "        # Check if all agents are done\n",
    "        done = all(terminations.values()) or all(truncations.values())\n",
    "    \n",
    "    # Append cumulative reward for the episode\n",
    "    episode_rewards.append(cumulative_reward)\n",
    "    \n",
    "    # Print rewards every 'print_interval' episodes\n",
    "    if episode % print_interval == 0:\n",
    "        avg_reward = sum(episode_rewards[-print_interval:]) / print_interval\n",
    "        print(f\"Episode {episode}: Average Reward: {avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctype:['adversary_0', 'adversary_1', 'adversary_2']\n",
      "ctype:['adversary_3']\n",
      "agent A:adversary_0\n",
      "agent B:adversary_3\n",
      "Models saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a directory to save models\n",
    "model_dir = 'saved_models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "\n",
    "def save_models(num_class_A, num_class_B, adversary_agents):\n",
    "    class_a_agents = adversary_agents[:num_class_A]\n",
    "    class_b_agents = adversary_agents[num_class_A:num_class_A + num_class_B]\n",
    "    for agent in class_a_agents:\n",
    "        torch.save(actor_networks[agent].state_dict(), os.path.join(model_dir, f\"actor_class_A.pth\"))\n",
    "        break  # Save only one model for Class A (assuming they share weights)\n",
    "    for agent in class_b_agents:\n",
    "        torch.save(actor_networks[agent].state_dict(), os.path.join(model_dir, f\"actor_class_B.pth\"))\n",
    "        break  # Save only one model for Class B (assuming they share weights)\n",
    "\n",
    "save_models(num_class_a, num_class_b, adversary_agents)\n",
    "# Save the GCN and Encoder models\n",
    "torch.save(gcn_layer.state_dict(), os.path.join(model_dir, \"gcn_model.pth\"))\n",
    "torch.save(encoder.state_dict(), os.path.join(model_dir, \"encoder_model.pth\"))\n",
    "\n",
    "print(\"Models saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Models and Test with Different Number of Adversaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x35 and 5x8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m final_input \u001b[38;5;241m=\u001b[39m final_inputs[agent]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension for LSTM processing\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Forward pass through the Encoder\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m hidden, cell \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Forward pass through the Decoder\u001b[39;00m\n\u001b[1;32m     49\u001b[0m embedding \u001b[38;5;241m=\u001b[39m decoder(hidden, cell)  \u001b[38;5;66;03m# Embedding will be passed to the GCN\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/harl/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[35], line 10\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# src: [batch_size, seq_len, input_dim]\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, seq_len, embed_dim]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     outputs, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(embedded)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# hidden, cell: [num_layers, batch_size, hidden_dim]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/harl/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/harl/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x35 and 5x8)"
     ]
    }
   ],
   "source": [
    "# Load the models\n",
    "# Define Encoder and GCN layer\n",
    "encoder = Encoder(input_dim=5, hidden_dim=hidden_dim)  # Adjust input_dim as needed\n",
    "encoder.load_state_dict(torch.load(os.path.join(model_dir, \"encoder_model.pth\")))\n",
    "\n",
    "gcn_layer = GCNLayer(input_dim=hidden_dim, output_dim=gcn_output_dim)\n",
    "gcn_layer.load_state_dict(torch.load(os.path.join(model_dir, \"gcn_model.pth\")))\n",
    "\n",
    "# Re-initialize Actor networks for new agents and load the saved models\n",
    "actor_networks = {}\n",
    "\n",
    "# You might test with a different number of adversaries here\n",
    "# For example, testing with 5 adversaries\n",
    "test_num_adversaries = 5\n",
    "test_num_class_a = 3\n",
    "test_num_class_b = 2\n",
    "\n",
    "# Initialize the environment with the new number of adversaries\n",
    "env = simple_tag_v2.parallel_env(render_mode=None, num_adversaries=test_num_adversaries, num_good=1, num_obstacles=2)\n",
    "env.reset()\n",
    "\n",
    "# Get the new lists of adversaries and good agents\n",
    "adversary_agents = [agent for agent in env.agents if 'adversary' in agent]\n",
    "good_agents = [agent for agent in env.agents if 'agent' in agent]\n",
    "\n",
    "# Initialize Actor networks for the new adversaries\n",
    "class_a_agents = adversary_agents[:test_num_class_a]\n",
    "class_b_agents = adversary_agents[test_num_class_a:test_num_class_a + test_num_class_b]\n",
    "\n",
    "for agent in adversary_agents:\n",
    "    input_dim = input_actor_network_max_dim  # Use the same padded input dim as before\n",
    "    output_dim = env.action_space(agent).n  # Assuming discrete action space\n",
    "    actor_net = ActorNetwork(input_dim, output_dim)\n",
    "    \n",
    "    # Load the appropriate saved model based on the class of the agent\n",
    "    if agent in class_a_agents:\n",
    "        actor_net.load_state_dict(torch.load(os.path.join(model_dir, f\"actor_class_A.pth\")))\n",
    "    else:\n",
    "        actor_net.load_state_dict(torch.load(os.path.join(model_dir, f\"actor_class_B.pth\")))\n",
    "    \n",
    "    actor_networks[agent] = actor_net\n",
    "\n",
    "# Run the environment for testing\n",
    "num_epochs = 50\n",
    "print_interval = 10  # Calculate average reward every 10 epochs\n",
    "episode_rewards = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    cumulative_reward = 0  # Reset cumulative reward for the epoch\n",
    "    observations = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Update adversary_agents and good_agents based on current observations\n",
    "        adversary_agents = [agent for agent in observations.keys() if 'adversary' in agent]\n",
    "        good_agents = [agent for agent in observations.keys() if 'agent' in agent]\n",
    "        \n",
    "        num_adversaries = len(adversary_agents)\n",
    "        num_good_agents = len(good_agents)\n",
    "        \n",
    "        # Apply the observation wrapper for adversaries\n",
    "        observations, final_inputs = adversary_observation_wrapper(\n",
    "            observations, test_num_class_a, test_num_class_b, adversary_agents, good_agents, num_adversaries, num_good_agents, num_obstacles\n",
    "        )\n",
    "\n",
    "        actions = {}\n",
    "        for agent in adversary_agents:\n",
    "            # Pad the input to match the maximum input dimension\n",
    "            m = nn.ConstantPad1d((0, input_actor_network_max_dim - final_inputs[agent].shape[0]), 0)\n",
    "            final_inputs_pad = m(final_inputs[agent])\n",
    "            \n",
    "            # Pass through the Actor network\n",
    "            action_probs = actor_networks[agent](final_inputs_pad)\n",
    "            action = torch.argmax(action_probs).item()  # Take the action with the highest probability\n",
    "            actions[agent] = action\n",
    "\n",
    "        # For good agents, sample random actions\n",
    "        for agent in good_agents:\n",
    "            actions[agent] = env.action_space(agent).sample()\n",
    "\n",
    "        # Step the environment\n",
    "        next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        # Update cumulative reward for this epoch\n",
    "        cumulative_reward += sum(rewards.values())\n",
    "\n",
    "        # Check if all agents are done\n",
    "        if all(terminations.values()) or all(truncations.values()):\n",
    "            done = True\n",
    "\n",
    "        # Update observations\n",
    "        observations = next_observations\n",
    "\n",
    "    # Track the cumulative reward for the epoch\n",
    "    episode_rewards.append(cumulative_reward)\n",
    "\n",
    "    # Print average reward every 'print_interval' epochs\n",
    "    if epoch % print_interval == 0:\n",
    "        avg_reward = sum(episode_rewards[-print_interval:]) / print_interval\n",
    "        print(f\"Epoch {epoch}: Average Reward over last {print_interval} epochs: {avg_reward}\")\n",
    "\n",
    "print(\"Test completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
