{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversary Agents: ['adversary_0', 'adversary_1', 'adversary_2', 'adversary_3']\n",
      "Good Agents: ['agent_0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.data as pyg_data\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "import os\n",
    "\n",
    "# Initialize the environment\n",
    "env = simple_tag_v2.parallel_env(render_mode=None, num_adversaries=4, num_good=1, num_obstacles=2)\n",
    "env.reset()\n",
    "\n",
    "# Parameters\n",
    "num_class_a = 3\n",
    "num_class_b = 1\n",
    "num_adversaries = 4\n",
    "num_good_agents = 1  # Only one agent being chased by adversaries\n",
    "num_obstacles = 2\n",
    "\n",
    "# Initial lists of agents\n",
    "adversary_agents = [agent for agent in env.agents if 'adversary' in agent]\n",
    "good_agents = [agent for agent in env.agents if 'agent' in agent]\n",
    "\n",
    "print(\"Adversary Agents:\", adversary_agents)\n",
    "print(\"Good Agents:\", good_agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Temporal GCN model\n",
    "class TemporalGCN(nn.Module):\n",
    "    def __init__(self, node_input_dim, hidden_dim, output_dim):\n",
    "        super(TemporalGCN, self).__init__()\n",
    "        # Spatial GCN layers\n",
    "        self.conv1 = pyg_nn.GCNConv(node_input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Temporal layer (e.g., GRU)\n",
    "        self.gru = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x_list, edge_index_list):\n",
    "        \"\"\"\n",
    "        x_list: List of node feature tensors for each timestep [num_nodes_t, node_input_dim]\n",
    "        edge_index_list: List of edge_index tensors for each timestep\n",
    "        \"\"\"\n",
    "        h_list = []\n",
    "        for x, edge_index in zip(x_list, edge_index_list):\n",
    "            # Spatial GCN\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = self.relu(h)\n",
    "            h_list.append(h)\n",
    "        \n",
    "        # Stack the node embeddings to form a sequence [batch_size, seq_len, hidden_dim]\n",
    "        # Here, batch_size is the number of nodes, seq_len is the number of timesteps\n",
    "        h_seq = torch.stack(h_list, dim=1)  # [num_nodes, seq_len, hidden_dim]\n",
    "        \n",
    "        # Temporal modeling\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(1, h_seq.size(0), self.gru.hidden_size).to(h_seq.device)\n",
    "        # Pass through GRU\n",
    "        out, hn = self.gru(h_seq, h0)  # out: [num_nodes, seq_len, hidden_dim]\n",
    "        # Take the last timestep's output\n",
    "        out = out[:, -1, :]  # [num_nodes, hidden_dim]\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc(out)  # [num_nodes, output_dim]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversary_observation_wrapper(observations, adversary_agents, good_agents):\n",
    "    updated_observations = {}\n",
    "    adversary_positions = {}  # Store adversary positions for graph construction\n",
    "    node_features = {}\n",
    "    \n",
    "    # For each adversary, extract self features\n",
    "    for agent in adversary_agents:\n",
    "        obs = observations[agent]\n",
    "        # Extract self_vel and self_pos (indices 0-3)\n",
    "        self_features = obs[0:4]\n",
    "        # Add class identifier (0 for Class A, 1 for Class B)\n",
    "        agent_class = 0 if agent in adversary_agents[:num_class_a] else 1\n",
    "        self_features = np.concatenate([self_features, [agent_class]])\n",
    "        # Convert to tensor\n",
    "        self_features = torch.tensor(self_features, dtype=torch.float32)\n",
    "        node_features[agent] = self_features\n",
    "        # Store position for graph construction (indices 2-3)\n",
    "        adversary_positions[agent] = obs[2:4]\n",
    "        # Update observations\n",
    "        updated_observations[agent] = obs  # Keep original observation if needed\n",
    "    \n",
    "    # Construct edge_index based on communication range\n",
    "    edge_index = []\n",
    "    agent_to_idx = {agent: idx for idx, agent in enumerate(adversary_agents)}\n",
    "    for agent in adversary_agents:\n",
    "        own_position = adversary_positions[agent]\n",
    "        for other_agent in adversary_agents:\n",
    "            if agent != other_agent:\n",
    "                other_position = adversary_positions[other_agent]\n",
    "                distance = np.linalg.norm(own_position - other_position)\n",
    "                if distance <= communication_range:\n",
    "                    edge_index.append([agent_to_idx[agent], agent_to_idx[other_agent]])\n",
    "    \n",
    "    # Convert edge_index to tensor\n",
    "    if edge_index:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    else:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    \n",
    "    # Convert node_features to a tensor\n",
    "    node_features_list = [node_features[agent] for agent in adversary_agents]\n",
    "    x = torch.stack(node_features_list)  # [num_nodes, node_input_dim]\n",
    "    \n",
    "    return updated_observations, x, edge_index, agent_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node input dimension: self_vel (2) + self_pos (2) + class_id (1) = 5\n",
    "node_input_dim = 5\n",
    "hidden_dim = 16\n",
    "communication_range = 1.5\n",
    "output_dim = env.action_space(adversary_agents[0]).n  # Assuming all adversaries have the same action space\n",
    "\n",
    "# Initialize the Temporal GCN model\n",
    "temporal_gcn = TemporalGCN(node_input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(temporal_gcn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss function (placeholder)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10: Average Reward: 10.7807294692779\n",
      "Episode 20: Average Reward: 8.530477074640222\n",
      "Episode 30: Average Reward: -6.904504160444003\n",
      "Episode 40: Average Reward: -13.201293840875223\n",
      "Episode 50: Average Reward: 3.500866183514612\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_episodes = 50  # Total number of episodes to run\n",
    "print_interval = 10  # Print rewards every 10 episodes\n",
    "\n",
    "# Initialize reward tracking\n",
    "episode_rewards = []\n",
    "\n",
    "# For temporal modeling, we'll collect sequences over timesteps\n",
    "sequence_length = 5  # Number of timesteps to consider in the temporal model\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    observations = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0  # Reset cumulative reward for the episode\n",
    "    \n",
    "    # Initialize lists to collect data over time\n",
    "    x_sequence = []  # List of node features for each timestep\n",
    "    edge_index_sequence = []  # List of edge indices for each timestep\n",
    "    \n",
    "    while not done:\n",
    "        # Update adversary_agents and good_agents based on current observations\n",
    "        adversary_agents = [agent for agent in observations.keys() if 'adversary' in agent]\n",
    "        good_agents = [agent for agent in observations.keys() if 'agent' in agent]\n",
    "        \n",
    "        # Apply the observation wrapper for adversaries\n",
    "        observations, x, edge_index, agent_to_idx = adversary_observation_wrapper(\n",
    "            observations, adversary_agents, good_agents)\n",
    "        \n",
    "        # Collect data for temporal modeling\n",
    "        x_sequence.append(x)\n",
    "        edge_index_sequence.append(edge_index)\n",
    "        \n",
    "        # Ensure we have enough timesteps for temporal modeling\n",
    "        if len(x_sequence) >= sequence_length:\n",
    "            # Use the last 'sequence_length' timesteps\n",
    "            x_input = x_sequence[-sequence_length:]\n",
    "            edge_index_input = edge_index_sequence[-sequence_length:]\n",
    "            \n",
    "            # Forward pass through Temporal GCN\n",
    "            output = temporal_gcn(x_input, edge_index_input)  # [num_nodes, output_dim]\n",
    "            \n",
    "            # Get actions\n",
    "            actions = {}\n",
    "            for agent in adversary_agents:\n",
    "                idx = agent_to_idx[agent]\n",
    "                action_probs = output[idx]\n",
    "                action = torch.argmax(action_probs).item()\n",
    "                actions[agent] = action\n",
    "            \n",
    "            # For good agents, sample random actions\n",
    "            for agent in good_agents:\n",
    "                actions[agent] = env.action_space(agent).sample()\n",
    "            \n",
    "            # Step the environment\n",
    "            next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "            \n",
    "            # Compute loss (placeholder)\n",
    "            target = torch.tensor([0] * len(adversary_agents), dtype=torch.long)  # Dummy target\n",
    "            loss = loss_fn(output, target)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update cumulative reward\n",
    "            cumulative_reward += sum(rewards.values())\n",
    "            \n",
    "            # Update observations\n",
    "            observations = next_observations\n",
    "            \n",
    "            # Check if all agents are done\n",
    "            done = all(terminations.values()) or all(truncations.values())\n",
    "        else:\n",
    "            # Not enough timesteps yet, take random actions\n",
    "            actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "            # Step the environment\n",
    "            observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "            # Update cumulative reward\n",
    "            cumulative_reward += sum(rewards.values())\n",
    "    \n",
    "    # Append cumulative reward for the episode\n",
    "    episode_rewards.append(cumulative_reward)\n",
    "    \n",
    "    # Print rewards every 'print_interval' episodes\n",
    "    if episode % print_interval == 0:\n",
    "        avg_reward = sum(episode_rewards[-print_interval:]) / print_interval\n",
    "        print(f\"Episode {episode}: Average Reward: {avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the Temporal GCN model\n",
    "model_dir = 'saved_models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "torch.save(temporal_gcn.state_dict(), os.path.join(model_dir, \"temporal_gcn_model.pth\"))\n",
    "\n",
    "print(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Average Reward over last 10 epochs: -14.06920927202702\n",
      "Epoch 20: Average Reward over last 10 epochs: 30.364071343836123\n",
      "Epoch 30: Average Reward over last 10 epochs: -3.5507463682510947\n",
      "Epoch 40: Average Reward over last 10 epochs: 5.822840216382039\n",
      "Epoch 50: Average Reward over last 10 epochs: 16.18777799676129\n",
      "Test completed.\n"
     ]
    }
   ],
   "source": [
    "# Testing with a different number of adversaries\n",
    "test_num_adversaries = 6  # Increased number of adversaries\n",
    "test_num_class_a = 4\n",
    "test_num_class_b = 2\n",
    "\n",
    "# Initialize the environment with the new number of adversaries\n",
    "env = simple_tag_v2.parallel_env(render_mode=None, num_adversaries=test_num_adversaries, num_good=1, num_obstacles=2)\n",
    "env.reset()\n",
    "\n",
    "# Run the environment for testing\n",
    "num_epochs = 50\n",
    "print_interval = 10  # Calculate average reward every 10 epochs\n",
    "episode_rewards = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    observations = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0  # Reset cumulative reward for the epoch\n",
    "    \n",
    "    x_sequence = []\n",
    "    edge_index_sequence = []\n",
    "    \n",
    "    while not done:\n",
    "        # Update adversary_agents and good_agents based on current observations\n",
    "        adversary_agents = [agent for agent in observations.keys() if 'adversary' in agent]\n",
    "        good_agents = [agent for agent in observations.keys() if 'agent' in agent]\n",
    "        \n",
    "        # Apply the observation wrapper for adversaries\n",
    "        observations, x, edge_index, agent_to_idx = adversary_observation_wrapper(\n",
    "            observations, adversary_agents, good_agents)\n",
    "        \n",
    "        # Collect data for temporal modeling\n",
    "        x_sequence.append(x)\n",
    "        edge_index_sequence.append(edge_index)\n",
    "        \n",
    "        if len(x_sequence) >= sequence_length:\n",
    "            x_input = x_sequence[-sequence_length:]\n",
    "            edge_index_input = edge_index_sequence[-sequence_length:]\n",
    "            \n",
    "            # Forward pass through Temporal GCN\n",
    "            output = temporal_gcn(x_input, edge_index_input)  # [num_nodes, output_dim]\n",
    "            \n",
    "            # Get actions\n",
    "            actions = {}\n",
    "            for agent in adversary_agents:\n",
    "                idx = agent_to_idx[agent]\n",
    "                action_probs = output[idx]\n",
    "                action = torch.argmax(action_probs).item()\n",
    "                actions[agent] = action\n",
    "            \n",
    "            # For good agents, sample random actions\n",
    "            for agent in good_agents:\n",
    "                actions[agent] = env.action_space(agent).sample()\n",
    "            \n",
    "            # Step the environment\n",
    "            observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "            \n",
    "            # Update cumulative reward\n",
    "            cumulative_reward += sum(rewards.values())\n",
    "            \n",
    "            # Check if all agents are done\n",
    "            if all(terminations.values()) or all(truncations.values()):\n",
    "                done = True\n",
    "        else:\n",
    "            # Not enough timesteps yet, take random actions\n",
    "            actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "            # Step the environment\n",
    "            observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "            # Update cumulative reward\n",
    "            cumulative_reward += sum(rewards.values())\n",
    "    \n",
    "    # Track the cumulative reward for the epoch\n",
    "    episode_rewards.append(cumulative_reward)\n",
    "    \n",
    "    # Print average reward every 'print_interval' epochs\n",
    "    if epoch % print_interval == 0:\n",
    "        avg_reward = sum(episode_rewards[-print_interval:]) / print_interval\n",
    "        print(f\"Epoch {epoch}: Average Reward over last {print_interval} epochs: {avg_reward}\")\n",
    "    \n",
    "print(\"Test completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
