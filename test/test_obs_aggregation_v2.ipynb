{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversary Agents: ['adversary_0', 'adversary_1', 'adversary_2', 'adversary_3']\n",
      "Good Agents: ['agent_0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.data as pyg_data\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "import os\n",
    "\n",
    "# Initialize the environment\n",
    "env = simple_tag_v2.parallel_env(render_mode=None, num_adversaries=4, num_good=1, num_obstacles=2)\n",
    "env.reset()\n",
    "\n",
    "# Parameters\n",
    "num_class_a = 3\n",
    "num_class_b = 1\n",
    "num_adversaries = 4\n",
    "num_good_agents = 1\n",
    "num_obstacles = 2\n",
    "communication_range = 10\n",
    "\n",
    "# Agent lists\n",
    "adversary_agents = [agent for agent in env.agents if 'adversary' in agent]\n",
    "good_agents = [agent for agent in env.agents if 'agent' in agent]\n",
    "landmarks = ['landmark_{}'.format(i) for i in range(num_obstacles)]\n",
    "\n",
    "print(\"Adversary Agents:\", adversary_agents)\n",
    "print(\"Good Agents:\", good_agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Temporal GCN model\n",
    "class TemporalGCN(nn.Module):\n",
    "    def __init__(self, node_input_dim, hidden_dim, output_dim):\n",
    "        super(TemporalGCN, self).__init__()\n",
    "        # Spatial GCN layers\n",
    "        self.conv1 = pyg_nn.GCNConv(node_input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Temporal layer (e.g., GRU)\n",
    "        self.gru = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x_list, edge_index_list):\n",
    "        \"\"\"\n",
    "        x_list: List of node feature tensors for each timestep [num_nodes_t, node_input_dim]\n",
    "        edge_index_list: List of edge_index tensors for each timestep\n",
    "        \"\"\"\n",
    "        h_list = []\n",
    "        for x, edge_index in zip(x_list, edge_index_list):\n",
    "            # Spatial GCN\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = self.relu(h)\n",
    "            h_list.append(h)\n",
    "        \n",
    "        # Stack the node embeddings to form a sequence [batch_size, seq_len, hidden_dim]\n",
    "        # Here, batch_size is the number of nodes, seq_len is the number of timesteps\n",
    "        h_seq = torch.stack(h_list, dim=1)  # [num_nodes, seq_len, hidden_dim]\n",
    "        \n",
    "        # Temporal modeling\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(1, h_seq.size(0), self.gru.hidden_size).to(h_seq.device)\n",
    "        # Pass through GRU\n",
    "        out, hn = self.gru(h_seq, h0)  # out: [num_nodes, seq_len, hidden_dim]\n",
    "        # Take the last timestep's output\n",
    "        out = out[:, -1, :]  # [num_nodes, hidden_dim]\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc(out)  # [num_nodes, output_dim]\n",
    "        return out\n",
    "    \n",
    "# Define the Graph Attention Network layer\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, heads=4):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.gat_conv = pyg_nn.GATConv(input_dim, output_dim, heads=heads, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.gat_conv(x, edge_index)\n",
    "    \n",
    "# Define Temporal Transformer\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, num_layers):\n",
    "        super(TemporalTransformer, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, h_seq):\n",
    "        # h_seq: [seq_len, num_nodes, hidden_dim]\n",
    "        h_seq = h_seq.permute(1, 0, 2)  # [num_nodes, seq_len, hidden_dim]\n",
    "        h_seq = h_seq.transpose(0, 1)    # Transformer expects [seq_len, batch_size, hidden_dim]\n",
    "        out = self.transformer(h_seq)\n",
    "        out = out[-1, :, :]  # Take the last timestep's output\n",
    "        return out  # [num_nodes, hidden_dim]\n",
    "    \n",
    "# Define the Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        action_probs = self.fc2(x)\n",
    "        return nn.functional.softmax(action_probs, dim=-1)\n",
    "\n",
    "# Define the complete model\n",
    "class AdvancedAgentModel(nn.Module):\n",
    "    def __init__(self, node_input_dim, hidden_dim, action_dim, num_heads=4, num_layers=2):\n",
    "        super(AdvancedAgentModel, self).__init__()\n",
    "        ## Change order of GATLayer and TemporalTransformer ##Maybe add Moro-san code here\n",
    "        # Spatial layer\n",
    "        self.gat = GATLayer(node_input_dim, hidden_dim, heads=num_heads)\n",
    "        # Temporal layer\n",
    "        self.temporal = TemporalTransformer(hidden_dim, num_heads, num_layers)\n",
    "        # Policy network\n",
    "        self.policy = PolicyNetwork(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x_list, edge_index_list):\n",
    "        h_list = []\n",
    "        for x, edge_index in zip(x_list, edge_index_list):\n",
    "            # Spatial GAT\n",
    "            h = self.gat(x, edge_index)\n",
    "            h_list.append(h)\n",
    "        # Stack the node embeddings to form a sequence\n",
    "        h_seq = torch.stack(h_list)  # [seq_len, num_nodes, hidden_dim]\n",
    "        # Temporal Transformer\n",
    "        out = self.temporal(h_seq)  # [num_nodes, hidden_dim]\n",
    "        # Policy network\n",
    "        action_probs = self.policy(out)  # [num_nodes, action_dim]\n",
    "        return action_probs\n",
    "    \n",
    "# Define the centralized critic\n",
    "class CentralizedCritic(nn.Module):\n",
    "    def __init__(self, node_input_dim, hidden_dim):\n",
    "        super(CentralizedCritic, self).__init__()\n",
    "        # GAT layer to process the global graph\n",
    "        self.gat = pyg_nn.GATConv(node_input_dim, hidden_dim, heads=4, concat=False)\n",
    "        # Self-attention layer\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4)\n",
    "        # Output layer\n",
    "        self.value_head = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # x: [num_nodes, node_input_dim]\n",
    "        # edge_index: [2, num_edges]\n",
    "        # batch: [num_nodes] (batch assignment for each node)\n",
    "        \n",
    "        # GAT layer\n",
    "        h = self.gat(x, edge_index)\n",
    "        h = nn.functional.elu(h)\n",
    "        \n",
    "        # Reshape for attention\n",
    "        h = h.unsqueeze(1)  # [num_nodes, 1, hidden_dim]\n",
    "        \n",
    "        # Self-attention\n",
    "        h, _ = self.attention(h, h, h)  # h: [num_nodes, 1, hidden_dim]\n",
    "        h = h.squeeze(1)  # [num_nodes, hidden_dim]\n",
    "        \n",
    "        # Global mean pooling\n",
    "        h = pyg_nn.global_mean_pool(h, batch)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Value estimation\n",
    "        value = self.value_head(h)  # [batch_size, 1]\n",
    "        return value.squeeze(-1)  # [batch_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversary_observation_wrapper(observations, adversary_agents, good_agents):\n",
    "    updated_observations = {}\n",
    "    adversary_positions = {}\n",
    "    node_features = {}\n",
    "    \n",
    "    for agent in adversary_agents:\n",
    "        obs = observations[agent]\n",
    "        # Self features: self_vel (2), self_pos (2), class_id (1)  need to change here (ToDo), should work with relative position/velocity\n",
    "        self_features = obs[0:4]\n",
    "        agent_class = 0 if agent in adversary_agents[:num_class_a] else 1\n",
    "        self_features = np.concatenate([self_features, [agent_class]])\n",
    "        self_features = torch.tensor(self_features, dtype=torch.float32)\n",
    "        node_features[agent] = self_features\n",
    "        adversary_positions[agent] = obs[2:4]\n",
    "        updated_observations[agent] = obs\n",
    "    \n",
    "    # Construct edge_index based on communication range\n",
    "    edge_index = []\n",
    "    agent_to_idx = {agent: idx for idx, agent in enumerate(adversary_agents)}\n",
    "    for agent in adversary_agents:\n",
    "        own_pos = adversary_positions[agent]\n",
    "        for other_agent in adversary_agents:\n",
    "            if agent != other_agent:\n",
    "                other_pos = adversary_positions[other_agent]\n",
    "                distance = np.linalg.norm(own_pos - other_pos)\n",
    "                if distance <= communication_range:\n",
    "                    edge_index.append([agent_to_idx[agent], agent_to_idx[other_agent]])\n",
    "    \n",
    "    # Convert edge_index and node_features to tensors\n",
    "    if edge_index:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    else:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    node_features_list = [node_features[agent] for agent in adversary_agents]\n",
    "    x = torch.stack(node_features_list)  # [num_nodes, node_input_dim]\n",
    "    \n",
    "    return updated_observations, x, edge_index, agent_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_global_state(observations, adversary_agents, good_agents):\n",
    "    # Collect positions and velocities\n",
    "    node_features = []\n",
    "    node_types = []\n",
    "    node_positions = []\n",
    "    edge_index = []\n",
    "    node_idx = 0\n",
    "    idx_map = {}\n",
    "    \n",
    "    # Adversaries\n",
    "    for agent in adversary_agents:\n",
    "        obs = observations[agent]\n",
    "        # Self_vel (0:2), Self_pos (2:4)\n",
    "        vel = obs[0:2]\n",
    "        pos = obs[2:4]\n",
    "        features = np.concatenate([pos, vel])\n",
    "        node_features.append(features)\n",
    "        node_types.append(0)  # Adversary\n",
    "        node_positions.append(pos)\n",
    "        idx_map[agent] = node_idx\n",
    "        node_idx += 1\n",
    "    \n",
    "    # Good agents\n",
    "    for agent in good_agents:\n",
    "        obs = observations[agent]\n",
    "        vel = obs[0:2]\n",
    "        pos = obs[2:4]\n",
    "        features = np.concatenate([pos, vel])\n",
    "        node_features.append(features)\n",
    "        node_types.append(1)  # Good agent\n",
    "        node_positions.append(pos)\n",
    "        idx_map[agent] = node_idx\n",
    "        node_idx += 1\n",
    "    \n",
    "    # Landmarks (if positions are accessible)\n",
    "    # Assuming we can get landmark positions from the environment\n",
    "    for i in range(num_obstacles):\n",
    "        # Here, we need to access the landmark positions from the environment\n",
    "        # Since the observations do not include landmark positions directly, we might need to modify the environment to expose this information\n",
    "        landmark_pos = env.world.landmarks[i].state.p_pos\n",
    "        features = np.concatenate([landmark_pos, np.zeros(2)])  # No velocity\n",
    "        node_features.append(features)\n",
    "        node_types.append(2)  # Landmark\n",
    "        node_positions.append(landmark_pos)\n",
    "        idx_map['landmark_{}'.format(i)] = node_idx\n",
    "        node_idx += 1\n",
    "    \n",
    "    # Build edge_index (fully connected for the critic)\n",
    "    num_nodes = len(node_features)\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:\n",
    "                edge_index.append([i, j])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Convert to tensors\n",
    "    x = torch.tensor(node_features, dtype=torch.float32)\n",
    "    \n",
    "    # Create batch (since we have a single graph, batch is zeros)\n",
    "    batch = torch.zeros(num_nodes, dtype=torch.long)\n",
    "    \n",
    "    return x, edge_index, batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node input dimension: self_vel (2) + self_pos (2) + class_id (1) = 5\n",
    "node_input_dim = 5\n",
    "hidden_dim = 64\n",
    "action_dim = env.action_space(adversary_agents[0]).n\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "\n",
    "# Initialize the model\n",
    "advanced_agent_model = AdvancedAgentModel(node_input_dim, hidden_dim, action_dim, num_heads, num_layers)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "advanced_agent_model.to(device)\n",
    "\n",
    "# Use DistributedDataParallel if multiple GPUs are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    advanced_agent_model = nn.DataParallel(advanced_agent_model)\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "learning_rate = 3e-4\n",
    "optimizer = optim.Adam(advanced_agent_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5000, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the value network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, node_input_dim, hidden_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(node_input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        value = self.fc2(x)\n",
    "        return value.squeeze(-1)  # [num_nodes]\n",
    "\n",
    "# Initialize the value network\n",
    "value_network = ValueNetwork(node_input_dim, hidden_dim).to(device)\n",
    "value_optimizer = optim.Adam(value_network.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'agent_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m advanced_agent_model(x_input, edge_index_input)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Compute value estimates from the critic using the global state\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m global_x, global_edge_index, global_batch \u001b[38;5;241m=\u001b[39m \u001b[43mextract_global_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madversary_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgood_agents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m global_x \u001b[38;5;241m=\u001b[39m global_x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     83\u001b[0m global_edge_index \u001b[38;5;241m=\u001b[39m global_edge_index\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[22], line 25\u001b[0m, in \u001b[0;36mextract_global_state\u001b[0;34m(observations, adversary_agents, good_agents)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Good agents\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m good_agents:\n\u001b[0;32m---> 25\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[43mobservations\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     26\u001b[0m     vel \u001b[38;5;241m=\u001b[39m obs[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     27\u001b[0m     pos \u001b[38;5;241m=\u001b[39m obs[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m4\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'agent_0'"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 1000  # Increase for better training\n",
    "max_timesteps = 200  # Max timesteps per episode\n",
    "gamma = 0.99         # Discount factor\n",
    "gae_lambda = 0.95    # GAE lambda for advantage computation\n",
    "epsilon = 0.2        # PPO clip parameter\n",
    "entropy_coef = 0.01  # Coefficient for entropy regularization\n",
    "value_coef = 0.5     # Coefficient for value loss\n",
    "grad_norm_clip = 0.5 # Gradient clipping\n",
    "\n",
    "# Storage for trajectories\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.edge_indices = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.global_states = []  # Global states for critic\n",
    "        self.global_edge_indices = []  # Edge indices for critic\n",
    "        self.global_batches = []  # Batches for critic\n",
    "\n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.edge_indices = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.global_states = []\n",
    "        self.global_edge_indices = []\n",
    "        self.global_batches = []\n",
    "\n",
    "buffer = RolloutBuffer()\n",
    "episode_rewards = deque(maxlen=100)\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    observations = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "    timestep = 0\n",
    "    buffer.clear()\n",
    "\n",
    "    x_sequence = []\n",
    "    edge_index_sequence = []\n",
    "\n",
    "    while not done and timestep < max_timesteps:\n",
    "        adversary_agents = [agent for agent in observations.keys() if 'adversary' in agent]\n",
    "        good_agents = [agent for agent in observations.keys() if 'agent' in agent]\n",
    "\n",
    "        # Observation wrapper\n",
    "        observations, x, edge_index, agent_to_idx = adversary_observation_wrapper(\n",
    "            observations, adversary_agents, good_agents)\n",
    "        x = x.to(device)\n",
    "        edge_index = edge_index.to(device)\n",
    "\n",
    "        # Collect data for temporal modeling\n",
    "        x_sequence.append(x)\n",
    "        edge_index_sequence.append(edge_index)\n",
    "\n",
    "        if len(x_sequence) > 5:\n",
    "            x_sequence.pop(0)\n",
    "            edge_index_sequence.pop(0)\n",
    "\n",
    "        # Prepare input sequences\n",
    "        x_input = x_sequence.copy()\n",
    "        edge_index_input = edge_index_sequence.copy()\n",
    "        for i in range(len(x_input)):\n",
    "            x_input[i] = x_input[i].to(device)\n",
    "            edge_index_input[i] = edge_index_input[i].to(device)\n",
    "\n",
    "        # Get action probabilities from the actor\n",
    "        action_probs = advanced_agent_model(x_input, edge_index_input)\n",
    "\n",
    "        # Compute value estimates from the critic using the global state\n",
    "        global_x, global_edge_index, global_batch = extract_global_state(observations, adversary_agents, good_agents)\n",
    "        global_x = global_x.to(device)\n",
    "        global_edge_index = global_edge_index.to(device)\n",
    "        global_batch = global_batch.to(device)\n",
    "        value_estimates = critic(global_x, global_edge_index, global_batch)\n",
    "\n",
    "        # Sample actions\n",
    "        actions = {}\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        for agent in adversary_agents:\n",
    "            idx = agent_to_idx[agent]\n",
    "            dist = torch.distributions.Categorical(action_probs[idx])\n",
    "            action = dist.sample()\n",
    "            actions[agent] = action.item()\n",
    "            log_probs.append(dist.log_prob(action))\n",
    "            values.append(value_estimates[idx])\n",
    "\n",
    "        # For good agents, sample random actions\n",
    "        for agent in good_agents:\n",
    "            actions[agent] = env.action_space(agent).sample()\n",
    "\n",
    "        # Step the environment\n",
    "        next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        # Store transitions\n",
    "        buffer.states.append(x)\n",
    "        buffer.edge_indices.append(edge_index)\n",
    "        buffer.actions.append(torch.tensor([actions[agent] for agent in adversary_agents], dtype=torch.long, device=device))\n",
    "        buffer.log_probs.append(torch.stack(log_probs))\n",
    "        buffer.rewards.append(torch.tensor([rewards[agent] for agent in adversary_agents], dtype=torch.float32, device=device))\n",
    "        buffer.values.append(torch.stack(values))\n",
    "        buffer.dones.append(torch.tensor([done]*len(adversary_agents), dtype=torch.float32, device=device))\n",
    "        buffer.global_states.append(global_x)\n",
    "        buffer.global_edge_indices.append(global_edge_index)\n",
    "        buffer.global_batches.append(global_batch)\n",
    "\n",
    "        cumulative_reward += sum(rewards.values())\n",
    "        observations = next_observations\n",
    "        done = all(terminations.values()) or all(truncations.values())\n",
    "        timestep += 1\n",
    "\n",
    "    # Compute advantages and returns using Generalized Advantage Estimation (GAE)\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    next_value = 0\n",
    "    for i in reversed(range(len(buffer.rewards))):\n",
    "        delta = buffer.rewards[i].mean() + gamma * next_value * (1 - buffer.dones[i].mean()) - buffer.values[i].mean()\n",
    "        gae = delta + gamma * gae_lambda * gae * (1 - buffer.dones[i].mean())\n",
    "        advantages.insert(0, gae)\n",
    "        returns.insert(0, gae + buffer.values[i].mean())\n",
    "        next_value = buffer.values[i].mean()\n",
    "\n",
    "    # Flatten the lists\n",
    "    states = torch.cat(buffer.states)\n",
    "    edge_indices = buffer.edge_indices\n",
    "    actions = torch.cat(buffer.actions)\n",
    "    log_probs_old = torch.cat(buffer.log_probs).detach()\n",
    "    returns = torch.cat(returns).detach()\n",
    "    advantages = torch.cat(advantages).detach()\n",
    "\n",
    "    # PPO update\n",
    "    num_mini_batch = 4\n",
    "    mini_batch_size = len(buffer.actions) // num_mini_batch\n",
    "\n",
    "    for _ in range(4):  # Optimize policy for K epochs\n",
    "        # Shuffle indices\n",
    "        indices = np.arange(len(buffer.actions))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for start in range(0, len(buffer.actions), mini_batch_size):\n",
    "            end = start + mini_batch_size\n",
    "            mb_indices = indices[start:end]\n",
    "\n",
    "            # Prepare minibatch data\n",
    "            mb_states = [buffer.states[i] for i in mb_indices]\n",
    "            mb_edge_indices = [buffer.edge_indices[i] for i in mb_indices]\n",
    "            mb_actions = torch.cat([buffer.actions[i] for i in mb_indices])\n",
    "            mb_log_probs_old = torch.cat([buffer.log_probs[i] for i in mb_indices]).detach()\n",
    "            mb_returns = torch.stack([returns[i] for i in mb_indices]).detach()\n",
    "            mb_advantages = torch.stack([advantages[i] for i in mb_indices]).detach()\n",
    "            mb_global_states = [buffer.global_states[i] for i in mb_indices]\n",
    "            mb_global_edge_indices = [buffer.global_edge_indices[i] for i in mb_indices]\n",
    "            mb_global_batches = [buffer.global_batches[i] for i in mb_indices]\n",
    "\n",
    "            # Compute action probabilities\n",
    "            action_probs = []\n",
    "            for x_input, edge_index_input in zip(mb_states, mb_edge_indices):\n",
    "                x_sequence = [x_input]\n",
    "                edge_index_sequence = [edge_index_input]\n",
    "                ap = advanced_agent_model(x_sequence, edge_index_sequence)\n",
    "                action_probs.append(ap)\n",
    "            action_probs = torch.cat(action_probs)\n",
    "\n",
    "            # Recompute value estimates using the critic\n",
    "            values = []\n",
    "            for global_x, global_edge_index, global_batch in zip(mb_global_states, mb_global_edge_indices, mb_global_batches):\n",
    "                value = critic(global_x, global_edge_index, global_batch)\n",
    "                values.append(value)\n",
    "            values = torch.stack(values)\n",
    "\n",
    "            # Compute new log probs and entropy\n",
    "            log_probs = []\n",
    "            entropy = 0\n",
    "            for idx in range(len(mb_actions)):\n",
    "                dist = torch.distributions.Categorical(action_probs[idx])\n",
    "                log_prob = dist.log_prob(mb_actions[idx])\n",
    "                log_probs.append(log_prob)\n",
    "                entropy += dist.entropy()\n",
    "\n",
    "            log_probs = torch.stack(log_probs)\n",
    "            ratios = torch.exp(log_probs - mb_log_probs_old)\n",
    "\n",
    "            # Surrogate loss\n",
    "            surr1 = ratios * mb_advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * mb_advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            value_loss = value_coef * (mb_returns - values).pow(2).mean()\n",
    "\n",
    "            # Entropy regularization\n",
    "            entropy_loss = -entropy_coef * entropy.mean()\n",
    "\n",
    "            # Total loss\n",
    "            loss = policy_loss + value_loss + entropy_loss\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            value_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(advanced_agent_model.parameters(), grad_norm_clip)\n",
    "            optimizer.step()\n",
    "            value_optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    episode_rewards.append(cumulative_reward)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        avg_reward = np.mean(episode_rewards)\n",
    "        print(f\"Episode {episode}: Average Reward: {avg_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a directory to save models\n",
    "model_dir = 'saved_models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Save the models\n",
    "torch.save(advanced_agent_model.state_dict(), os.path.join(model_dir, \"advanced_agent_model.pth\"))\n",
    "torch.save(value_network.state_dict(), os.path.join(model_dir, \"value_network.pth\"))\n",
    "print(\"Models saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Average Reward over last 10 epochs: -9.548204535002139\n",
      "Epoch 20: Average Reward over last 10 epochs: 52.82452114051563\n",
      "Epoch 30: Average Reward over last 10 epochs: 11.951303239275566\n",
      "Epoch 40: Average Reward over last 10 epochs: 5.262679379053028\n",
      "Epoch 50: Average Reward over last 10 epochs: 26.979216094949738\n",
      "Test completed.\n"
     ]
    }
   ],
   "source": [
    "# Load the models\n",
    "advanced_agent_model.load_state_dict(torch.load(os.path.join(model_dir, \"advanced_agent_model.pth\")))\n",
    "critic.load_state_dict(torch.load(os.path.join(model_dir, \"critic_model.pth\")))\n",
    "\n",
    "# Set models to evaluation mode\n",
    "advanced_agent_model.eval()\n",
    "critic.eval()\n",
    "\n",
    "# Testing parameters\n",
    "test_num_adversaries = 6  # Increase number of adversaries\n",
    "env = simple_tag_v2.parallel_env(render_mode=None, num_adversaries=test_num_adversaries, num_good=1, num_obstacles=2)\n",
    "env.reset()\n",
    "num_test_episodes = 50\n",
    "test_episode_rewards = []\n",
    "\n",
    "for episode in range(num_test_episodes):\n",
    "    observations = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "    x_sequence = []\n",
    "    edge_index_sequence = []\n",
    "\n",
    "    while not done:\n",
    "        adversary_agents = [agent for agent in observations.keys() if 'adversary' in agent]\n",
    "        good_agents = [agent for agent in observations.keys() if 'agent' in agent]\n",
    "\n",
    "        # Observation wrapper\n",
    "        observations, x, edge_index, agent_to_idx = adversary_observation_wrapper(\n",
    "            observations, adversary_agents, good_agents)\n",
    "        x = x.to(device)\n",
    "        edge_index = edge_index.to(device)\n",
    "\n",
    "        x_sequence.append(x)\n",
    "        edge_index_sequence.append(edge_index)\n",
    "\n",
    "        if len(x_sequence) > 5:\n",
    "            x_sequence.pop(0)\n",
    "            edge_index_sequence.pop(0)\n",
    "\n",
    "        x_input = x_sequence.copy()\n",
    "        edge_index_input = edge_index_sequence.copy()\n",
    "        for i in range(len(x_input)):\n",
    "            x_input[i] = x_input[i].to(device)\n",
    "            edge_index_input[i] = edge_index_input[i].to(device)\n",
    "\n",
    "        # Get action probabilities\n",
    "        with torch.no_grad():\n",
    "            action_probs = advanced_agent_model(x_input, edge_index_input)\n",
    "\n",
    "        # Select actions\n",
    "        actions = {}\n",
    "        for agent in adversary_agents:\n",
    "            idx = agent_to_idx[agent]\n",
    "            action = torch.argmax(action_probs[idx]).item()\n",
    "            actions[agent] = action\n",
    "\n",
    "        # Good agents take random actions\n",
    "        for agent in good_agents:\n",
    "            actions[agent] = env.action_space(agent).sample()\n",
    "\n",
    "        # Step the environment\n",
    "        observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        cumulative_reward += sum(rewards.values())\n",
    "        done = all(terminations.values()) or all(truncations.values())\n",
    "\n",
    "    test_episode_rewards.append(cumulative_reward)\n",
    "\n",
    "avg_test_reward = np.mean(test_episode_rewards)\n",
    "print(f\"Average Test Reward over {num_test_episodes} episodes: {avg_test_reward:.2f}\")\n",
    "print(\"Testing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
