{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversary Agents: ['adversary_0', 'adversary_1', 'adversary_2', 'adversary_3']\n",
      "Good Agents: ['agent_0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "import os\n",
    "\n",
    "# Initialize the environment\n",
    "env = simple_tag_v2.parallel_env(render_mode=None, num_adversaries=4, num_good=1, num_obstacles=2)\n",
    "env.reset()\n",
    "\n",
    "# Parameters\n",
    "num_class_a = 3\n",
    "num_class_b = 1\n",
    "num_adversaries = 4\n",
    "num_agents = 1  # Only one agent being chased by adversaries\n",
    "num_obstacles = 2\n",
    "adversary_agents = [agent for agent in env.agents if 'adversary' in agent]\n",
    "good_agents = [agent for agent in env.agents if 'agent' in agent]\n",
    "\n",
    "print(\"Adversary Agents:\", adversary_agents)\n",
    "print(\"Good Agents:\", good_agents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Embedding and CGN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define linear embedding layer\n",
    "class LinearEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim):\n",
    "        super(LinearEmbedding, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Define GCN layer\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.conv1 = pyg_nn.GCNConv(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        return self.conv1(x, edge_index)\n",
    "\n",
    "# Parameters for embedding and GCN\n",
    "communication_range = 1.5  # Define the communication range\n",
    "embedding_dim = 8          # Dimension for linear embedding\n",
    "gcn_output_dim = 16        # Dimension of GCN output\n",
    "\n",
    "# Initialize embedding and GCN layers\n",
    "embedding_layer = LinearEmbedding(input_dim=5, embed_dim=embedding_dim)  # input_dim includes the class identifier\n",
    "gcn_layer = GCNLayer(input_dim=embedding_dim, output_dim=gcn_output_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversary_observation_wrapper(observations, num_class_A, num_class_B, adversary_agents, num_adversaries, num_agents, num_obstacles):\n",
    "    assert num_class_A + num_class_B == num_adversaries, \"Number of agents assigned to Class A and Class B must match the total number of adversaries.\"\n",
    "    \n",
    "    class_a_agents = adversary_agents[:num_class_A]\n",
    "    class_b_agents = adversary_agents[num_class_A:num_class_A + num_class_B]\n",
    "    \n",
    "    updated_observations = {}\n",
    "    adversary_positions = {}  # Store adversary positions for communication\n",
    "    \n",
    "    # Step 1: Add agent class identifier into observation\n",
    "    for agent, obs in observations.items():\n",
    "        # If the agent is an adversary (either Class A or Class B)\n",
    "        if agent in class_a_agents or agent in class_b_agents:\n",
    "            agent_class = 0 if agent in class_a_agents else 1  # Class A: 0, Class B: 1\n",
    "            updated_obs = np.concatenate([obs, [agent_class]])  # Add class identifier\n",
    "        else:\n",
    "            updated_obs = obs  # Non-adversary agents keep their observation\n",
    "        updated_observations[agent] = updated_obs\n",
    "    \n",
    "    # Step 2: Gather positions for communication\n",
    "    for agent in adversary_agents:\n",
    "        position = updated_observations[agent][2:4]  # Assume position is at index 2:4\n",
    "        adversary_positions[agent] = position\n",
    "    \n",
    "    # Step 3: Apply linear embedding and handle communication within range\n",
    "    embedded_information = {}\n",
    "    for agent, obs in updated_observations.items():\n",
    "        if agent in adversary_agents:\n",
    "            # Convert observation to torch tensor\n",
    "            obs_tensor = torch.tensor(obs[:5], dtype=torch.float32)  # Include the class identifier\n",
    "            embedded_obs = embedding_layer(obs_tensor)  # Apply embedding to observation\n",
    "            embedded_information[agent] = embedded_obs\n",
    "    \n",
    "    # Step 4: Communication and gather data for GCN input\n",
    "    node_features = []\n",
    "    edge_index = []\n",
    "    num_agents_in_graph = len(adversary_agents)\n",
    "    \n",
    "    agent_to_idx = {agent: idx for idx, agent in enumerate(adversary_agents)}\n",
    "    \n",
    "    for agent in adversary_agents:\n",
    "        own_position = adversary_positions[agent]\n",
    "        node_features.append(embedded_information[agent])\n",
    "        \n",
    "        for other_agent in adversary_agents:\n",
    "            if agent != other_agent:\n",
    "                other_position = adversary_positions[other_agent]\n",
    "                distance = np.linalg.norm(own_position - other_position)\n",
    "                \n",
    "                if distance <= communication_range:  # Check if within communication range\n",
    "                    edge_index.append([agent_to_idx[agent], agent_to_idx[other_agent]])  # Add edge\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    if edge_index:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    else:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    \n",
    "    node_features = torch.stack(node_features)\n",
    "    \n",
    "    # Step 5: Pass the gathered information into a GCN\n",
    "    gcn_output = gcn_layer(node_features, edge_index)\n",
    "    \n",
    "    # Step 6: Concatenate GCN output with agent's own observation\n",
    "    final_inputs = {}\n",
    "    for agent_idx, agent in enumerate(adversary_agents):\n",
    "        own_obs = torch.tensor(updated_observations[agent], dtype=torch.float32)\n",
    "        final_input = torch.cat([own_obs, gcn_output[agent_idx]])\n",
    "        final_inputs[agent] = final_input  # This will be used as input to the Actor network\n",
    "    \n",
    "    return updated_observations, final_inputs  # Return the updated observations and inputs for Actor network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Environment and Run a Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Observations:\n",
      "adversary_0: [ 0.30000001 -0.         -0.92546493 -0.7244485   0.55810302  0.99061686\n",
      "  1.54220319  1.34548914  0.54744869  0.61472976  0.81065941 -0.13042034\n",
      "  0.62077808  0.22459681  1.27130783  0.2539244   0.          0.\n",
      "  0.        ]\n",
      "adversary_1: [-0.30000001 -0.         -0.3780162  -0.10971878  0.01065429  0.3758871\n",
      "  0.99475449  0.73075944 -0.54744869 -0.61472976  0.26321071 -0.74515009\n",
      "  0.07332934 -0.39013293  0.72385913 -0.36080533  0.          0.\n",
      "  0.        ]\n",
      "adversary_2: [ 0.         -0.         -0.11480551 -0.85486889 -0.25255641  1.12103724\n",
      "  0.73154378  1.47590959 -0.81065941  0.13042034 -0.26321071  0.74515009\n",
      " -0.18988135  0.35501716  0.46064845  0.38434476  0.          0.\n",
      "  0.        ]\n",
      "adversary_3: [ 0.30000001 -0.         -0.30468687 -0.4998517  -0.06267506  0.76602\n",
      "  0.9214251   1.12089241 -0.62077808 -0.22459681 -0.07332934  0.39013293\n",
      "  0.18988135 -0.35501716  0.6505298   0.0293276   0.          0.\n",
      "  1.        ]\n",
      "agent_0: [ 0.          0.          0.34584293 -0.47052413 -0.71320486  0.7366924\n",
      "  0.27089533  1.0915648  -1.2713078  -0.2539244  -0.72385913  0.36080533\n",
      " -0.46064845 -0.38434476 -0.6505298  -0.0293276 ]\n",
      "\n",
      "Final Inputs for Actor Network:\n",
      "adversary_0: tensor([ 0.3000, -0.0000, -0.9255, -0.7244,  0.5581,  0.9906,  1.5422,  1.3455,\n",
      "         0.5474,  0.6147,  0.8107, -0.1304,  0.6208,  0.2246,  1.2713,  0.2539,\n",
      "         0.0000,  0.0000,  0.0000,  0.0578,  0.1325,  0.0113,  0.0196, -0.1518,\n",
      "        -0.0090, -0.0683,  0.1921, -0.0049,  0.2178,  0.2077, -0.1658,  0.0923,\n",
      "        -0.1838, -0.1278, -0.1201], grad_fn=<CatBackward0>)\n",
      "adversary_1: tensor([-0.3000, -0.0000, -0.3780, -0.1097,  0.0107,  0.3759,  0.9948,  0.7308,\n",
      "        -0.5474, -0.6147,  0.2632, -0.7452,  0.0733, -0.3901,  0.7239, -0.3608,\n",
      "         0.0000,  0.0000,  0.0000,  0.0578,  0.1325,  0.0113,  0.0196, -0.1518,\n",
      "        -0.0090, -0.0683,  0.1921, -0.0049,  0.2178,  0.2077, -0.1658,  0.0923,\n",
      "        -0.1838, -0.1278, -0.1201], grad_fn=<CatBackward0>)\n",
      "adversary_2: tensor([ 0.0000, -0.0000, -0.1148, -0.8549, -0.2526,  1.1210,  0.7315,  1.4759,\n",
      "        -0.8107,  0.1304, -0.2632,  0.7452, -0.1899,  0.3550,  0.4606,  0.3843,\n",
      "         0.0000,  0.0000,  0.0000,  0.0578,  0.1325,  0.0113,  0.0196, -0.1518,\n",
      "        -0.0090, -0.0683,  0.1921, -0.0049,  0.2178,  0.2077, -0.1658,  0.0923,\n",
      "        -0.1838, -0.1278, -0.1201], grad_fn=<CatBackward0>)\n",
      "adversary_3: tensor([ 0.3000, -0.0000, -0.3047, -0.4999, -0.0627,  0.7660,  0.9214,  1.1209,\n",
      "        -0.6208, -0.2246, -0.0733,  0.3901,  0.1899, -0.3550,  0.6505,  0.0293,\n",
      "         0.0000,  0.0000,  1.0000,  0.0578,  0.1325,  0.0113,  0.0196, -0.1518,\n",
      "        -0.0090, -0.0683,  0.1921, -0.0049,  0.2178,  0.2077, -0.1658,  0.0923,\n",
      "        -0.1838, -0.1278, -0.1201], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment\n",
    "env.reset()\n",
    "\n",
    "# Sample action spaces for all agents\n",
    "actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "# Step through the environment\n",
    "observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "# Apply the observation wrapper for adversaries\n",
    "observations, final_inputs = adversary_observation_wrapper(\n",
    "    observations, num_class_a, num_class_b, adversary_agents, num_adversaries, num_agents, num_obstacles)\n",
    "\n",
    "print(\"Updated Observations:\")\n",
    "for agent, obs in observations.items():\n",
    "    print(f\"{agent}: {obs}\")\n",
    "\n",
    "print(\"\\nFinal Inputs for Actor Network:\")\n",
    "for agent, inp in final_inputs.items():\n",
    "    print(f\"{agent}: {inp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, output_dim)  # Output dimension should match the action space\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize Actor networks for adversaries\n",
    "actor_networks = {}\n",
    "for agent in adversary_agents:\n",
    "    input_dim = len(env.observation_space(agent).low) + 1 + gcn_output_dim  # Observation length plus class id plus GCN output\n",
    "    output_dim = env.action_space(agent).n  # Assuming discrete action space\n",
    "    actor_networks[agent] = ActorNetwork(input_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yungisimon/anaconda3/envs/harl/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10: Average Reward: 7.276664328311708\n",
      "Episode 20: Average Reward: -5.956658382421368\n",
      "Episode 30: Average Reward: 7.3109924135416025\n",
      "Episode 40: Average Reward: -7.55640975848721\n",
      "Episode 50: Average Reward: 12.434964790954348\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_episodes = 50  # Total number of episodes to run\n",
    "print_interval = 10  # Print rewards every 10 episodes\n",
    "\n",
    "# Initialize reward tracking\n",
    "episode_rewards = []\n",
    "cumulative_reward = 0\n",
    "\n",
    "# Optimizers for Actor networks and GCN (assuming we are training them)\n",
    "learning_rate = 0.001\n",
    "actor_optimizers = {agent: torch.optim.Adam(actor_networks[agent].parameters(), lr=learning_rate) for agent in adversary_agents}\n",
    "gcn_optimizer = torch.optim.Adam(gcn_layer.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss function (placeholder, you need to define based on your RL algorithm)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    observations = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0  # Reset cumulative reward for the episode\n",
    "    \n",
    "    while not done:\n",
    "        # Apply the observation wrapper for adversaries\n",
    "        observations, final_inputs = adversary_observation_wrapper(\n",
    "            observations, num_class_a, num_class_b, adversary_agents, num_adversaries, num_agents, num_obstacles)\n",
    "        \n",
    "        actions = {}\n",
    "        for agent in env.agents:\n",
    "            if agent in adversary_agents:\n",
    "                # Get the input for the Actor network\n",
    "                actor_input = final_inputs[agent]\n",
    "                # Get action probabilities (assuming discrete action space)\n",
    "                action_probs = actor_networks[agent](actor_input)\n",
    "                # Sample an action (for simplicity, we take the action with the highest probability)\n",
    "                action = torch.argmax(action_probs).item()\n",
    "                actions[agent] = action\n",
    "            else:\n",
    "                # For non-adversary agents, sample random actions\n",
    "                actions[agent] = env.action_space(agent).sample()\n",
    "        \n",
    "        # Step the environment\n",
    "        next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "        \n",
    "        # Update cumulative reward\n",
    "        cumulative_reward += sum(rewards.values())\n",
    "        \n",
    "        # Placeholder for training step (you need to implement your RL algorithm here)\n",
    "        # For example, compute loss and update networks\n",
    "        \n",
    "        # For simplicity, let's assume we have a target value (dummy value here)\n",
    "        target = torch.zeros(1)\n",
    "        loss = 0\n",
    "        for agent in adversary_agents:\n",
    "            # Get the predicted value\n",
    "            actor_input = final_inputs[agent]\n",
    "            prediction = actor_networks[agent](actor_input)\n",
    "            # Compute loss (this is a placeholder)\n",
    "            loss += loss_fn(prediction.unsqueeze(0), target)\n",
    "        \n",
    "        # Backpropagation\n",
    "        gcn_optimizer.zero_grad()\n",
    "        for optimizer in actor_optimizers.values():\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        gcn_optimizer.step()\n",
    "        for optimizer in actor_optimizers.values():\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Update observations\n",
    "        observations = next_observations\n",
    "        \n",
    "        # Check if all agents are done\n",
    "        done = all(terminations.values()) or all(truncations.values())\n",
    "    \n",
    "    # Append cumulative reward for the episode\n",
    "    episode_rewards.append(cumulative_reward)\n",
    "    \n",
    "    # Print rewards every 'print_interval' episodes\n",
    "    if episode % print_interval == 0:\n",
    "        avg_reward = sum(episode_rewards[-print_interval:]) / print_interval\n",
    "        print(f\"Episode {episode}: Average Reward: {avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctype:['adversary_0', 'adversary_1', 'adversary_2']\n",
      "ctype:['adversary_3']\n",
      "agent A:adversary_0\n",
      "agent B:adversary_3\n",
      "Models saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a directory to save models\n",
    "model_dir = 'saved_models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "\n",
    "def save_models(num_class_A, num_class_B, adversary_agents):\n",
    "    class_a_agents = adversary_agents[:num_class_A]\n",
    "    class_b_agents = adversary_agents[num_class_A:num_class_A + num_class_B]\n",
    "    print(f'ctype:{class_a_agents}')\n",
    "    print(f'ctype:{class_b_agents}')\n",
    "    for agent in class_a_agents:\n",
    "        if agent in class_a_agents:\n",
    "            print(f\"agent A:{agent}\")\n",
    "            torch.save(actor_networks[agent].state_dict(), os.path.join(model_dir, f\"actor_class_A.pth\"))\n",
    "            break\n",
    "    for agent in class_b_agents:\n",
    "        if agent in class_b_agents:\n",
    "            print(f\"agent B:{agent}\")\n",
    "            torch.save(actor_networks[agent].state_dict(), os.path.join(model_dir, f\"actor_class_B.pth\"))\n",
    "            break\n",
    "\n",
    "    #torch.save(class_a_agents[0].state_dict(), os.path.join(model_dir, f\"actor_class_A.pth\"))\n",
    "    #torch.save(class_b_agents[0].state_dict(), os.path.join(model_dir, f\"actor_class_B.pth\"))\n",
    "\n",
    "\n",
    "    # # Step 1: Add agent class identifier into observation\n",
    "    # for agent, obs in observations.items():\n",
    "    #     # If the agent is an adversary (either Class A or Class B)\n",
    "    #     if agent in class_a_agents or agent in class_b_agents:\n",
    "    #         agent_class = 0 if agent in class_a_agents else 1  # Class A: 0, Class B: 1\n",
    "    #         updated_obs = np.concatenate([obs, [agent_class]])  # Add class identifier\n",
    "    #     else:\n",
    "    #         updated_obs = obs  # Non-adversary agents keep their observation\n",
    "    #     updated_observations[agent] = updated_obs\n",
    "\n",
    "save_models(num_class_a, num_class_b, adversary_agents)\n",
    "\n",
    "# Save Actor networks\n",
    "#for i in range(1):\n",
    "#    torch.save(actor_networks[agent].state_dict(), os.path.join(model_dir, f\"actor_{agent}.pth\"))\n",
    "\n",
    "# Save GCN model\n",
    "torch.save(gcn_layer.state_dict(), os.path.join(model_dir, \"gcn_model.pth\"))\n",
    "\n",
    "print(\"Models saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Models and Test with Different Number of Adversaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: adversary_0\n",
      "Agent: adversary_0 class A\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ActorNetwork:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([64, 35]) from checkpoint, the shape in current model is torch.Size([64, 39]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m class_a_agents:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m class A\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mactor_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactor_class_A.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     actor_networks[agent] \u001b[38;5;241m=\u001b[39m actor_net\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m class_b_agents:\n",
      "File \u001b[0;32m~/anaconda3/envs/harl/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ActorNetwork:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([64, 35]) from checkpoint, the shape in current model is torch.Size([64, 39])."
     ]
    }
   ],
   "source": [
    "# Set up the environment with a different number of adversaries\n",
    "new_num_adversaries = 6  # Change the number of adversaries\n",
    "env = simple_tag_v2.parallel_env(render_mode=None, num_adversaries=new_num_adversaries, num_good=1, num_obstacles=2)\n",
    "env.reset()\n",
    "\n",
    "# Update adversary agents list\n",
    "adversary_agents = [agent for agent in env.agents if 'adversary' in agent]\n",
    "good_agents = [agent for agent in env.agents if 'agent' in agent]\n",
    "\n",
    "# Re-initialize embedding and GCN layers\n",
    "embedding_layer = LinearEmbedding(input_dim=5, embed_dim=embedding_dim)  # Same as before\n",
    "gcn_layer = GCNLayer(input_dim=embedding_dim, output_dim=gcn_output_dim)\n",
    "gcn_layer.load_state_dict(torch.load(os.path.join(model_dir, \"gcn_model.pth\")))\n",
    "\n",
    "class_a_agents = ['adversary_0', 'adversary_1', 'adversary_2', 'adversary_3']\n",
    "class_b_agents = ['adversary_4', 'adversary_5']\n",
    "\n",
    "# Re-initialize Actor networks for new agents and load the saved models\n",
    "actor_networks = {}\n",
    "for agent in adversary_agents:\n",
    "    print(f\"Agent: {agent}\")\n",
    "    input_dim = len(env.observation_space(agent).low) + 1 + gcn_output_dim  # Adjust if observation space changes\n",
    "    output_dim = env.action_space(agent).n  # Assuming discrete action space\n",
    "    actor_net = ActorNetwork(input_dim, output_dim)\n",
    "    # Load the saved model (using the first saved model for simplicity)\n",
    "    if agent in class_a_agents:\n",
    "        print(f\"Agent: {agent} class A\")\n",
    "        actor_net.load_state_dict(torch.load(os.path.join(model_dir, f\"actor_class_A.pth\")))\n",
    "        actor_networks[agent] = actor_net\n",
    "    if agent in class_b_agents:\n",
    "        print(f\"Agent: {agent} class B\")\n",
    "        actor_net.load_state_dict(torch.load(os.path.join(model_dir, f\"actor_class_B.pth\")))\n",
    "        actor_networks[agent] = actor_net\n",
    "\n",
    "    # Adjust the logic as per your agent classes\n",
    "    #saved_agent = 'adversary_0' if 'adversary_0' in agent or 'adversary_1' in agent or 'adversary_2' in agent else 'adversary_3'\n",
    "    #actor_net.load_state_dict(torch.load(os.path.join(model_dir, f\"actor_{saved_agent}.pth\")))\n",
    "    #actor_networks[agent] = actor_net\n",
    "\n",
    "print(\"Models loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
